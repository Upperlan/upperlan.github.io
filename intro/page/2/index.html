<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Apodidae">
<meta property="og:type" content="website">
<meta property="og:title" content="Upperlan">
<meta property="og:url" content="http://example.com/intro/page/2/index.html">
<meta property="og:site_name" content="Upperlan">
<meta property="og:description" content="Apodidae">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="篮球架上打砖块">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/intro/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Upperlan</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Upperlan</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/home/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/07/Vision-Transformer%E5%A4%84%E7%90%86%E6%81%B6%E6%84%8F%E8%BD%AF%E4%BB%B6%E5%9B%BE%E5%83%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="篮球架上打砖块">
      <meta itemprop="description" content="Apodidae">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Upperlan">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/07/Vision-Transformer%E5%A4%84%E7%90%86%E6%81%B6%E6%84%8F%E8%BD%AF%E4%BB%B6%E5%9B%BE%E5%83%8F/" class="post-title-link" itemprop="url">Vision Transformer处理恶意软件图像</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-03-07 12:06:39 / 修改时间：20:43:10" itemprop="dateCreated datePublished" datetime="2022-03-07T12:06:39+08:00">2022-03-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94%E4%B8%8E%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">科研与论文</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="整体思路"><a href="#整体思路" class="headerlink" title="整体思路"></a>整体思路</h1><p>将恶意软件的PE十六进制机器码转换成序列，再根据ASCII码表将序列值转化成像素点的数值，最后用多种方法填充成一幅图像，供Vision Transformer模型进行分类。</p>
<h1 id="1-简要前言：转换成图像的意义"><a href="#1-简要前言：转换成图像的意义" class="headerlink" title="1.简要前言：转换成图像的意义"></a>1.简要前言：转换成图像的意义</h1><p>可以利用视觉领域先进的模型与算法，更好的提取PE恶意软件十六进制代码里的特征。</p>
<p><img src="image-20220307121458807-6626500.png" alt="image-20220307121458807"></p>
<p>上图就是我们直接读取可执行PE文件的十六进制代码的结果。最左边一列是这些代码在虚拟内存中的地址，右边是每个地址存储的待执行代码。很明显几乎不可能直观的看出来这些代码的逻辑和操作。而如果用机器学习进行分类，就需要很多关于此类数据的先验知识对这些数据进行特征提取或降维。而深度学习模型的一个特点就是能够找到众多输入数据中所包含的特征，如图像分类问题中，CNN有能力识别到指定图片中目标对象的类别。因此直觉上深度学习模型是处理此类问题十分有效的工具。</p>
<p>而深度学习模型的输入数据一般是统一成一个指定类型的数据。目前图像领域里有很多sota模型可以对图像类数据的特征进行提取，分类，因此如果能让输入数据具备类似图像上的特征，即像素点数据分布的特征，那么就可以利用视觉类深度学习模型来处理恶意软件的分类问题。所以如何将恶意软件进行图像化就是一个很重要的问题了。</p>
<p>使用视觉类DL模型的另外一个原因是恶意软件的序列并不是严格意义上的时间序列，而是包含了丰富的语义特征的序列，就像NLP里单词和单词之间，单词与句子之间的语义一样。用于NLP模型的恶意软件数据集很难构建出来，因为需要大量的可执行文件反汇编，逆向工程的先验知识。因此图像类深度学习模型，或者由NLP领域发展而来的Vision Transformer模型是目前我实验中比较有效的模型。</p>
<h1 id="2-Vision-Transformer特点"><a href="#2-Vision-Transformer特点" class="headerlink" title="2.Vision Transformer特点"></a>2.Vision Transformer特点</h1><p>Vision Transformer模型的特点是和self attention模块的功能高度相关的。从宏观上来说，self-attention layer能够并行输入一个sequence，输出一个sequence，<strong>它的每一个输出都看过了整个的输入sequence，这一点与bi-directional RNN相同。但是神奇的地方是：它的每一个输出都可以并行化计算。</strong></p>
<p><img src="image-20220307134209708-6631731.png" alt="image-20220307134209708"></p>
<p>用CNN，你确实也可以做到输入是一个sequence，输出是另外一个sequence。但是，表面上CNN和Vision Transformer可以做到相同的输入和输出，但是CNN只能考虑非常有限的内容。比如在我们下图中CNN的filter只考虑了3个vector，不像RNN可以考虑之前的所有vector。但是CNN也不是没有办法考虑很长时间的dependency的，你只需要堆叠filter，多堆叠几层，上层的filter就可以考虑比较多的资讯，比如，第二层的filter (蓝色的三角形)看了6个vector，所以，只要叠很多层，就能够看很长时间的资讯。</p>
<p>做CNN的时候是只考虑感受野红框里面的资讯，而不是图片的全局信息。所以CNN可以看作是一种简化版本的self-attention。</p>
<p>或者可以反过来说，self-attention是一种复杂化的CNN，在做CNN的时候是只考虑感受野红框里面的资讯，而感受野的范围和大小是由人决定的。但是self-attention由attention找到相关的pixel，就好像是感受野的范围和大小是自动被学出来的，所以CNN可以看做是self-attention的特例，如图所示。</p>
<p><img src="v2-f28a8b0295863ab78d92a281ae55fce2_720w.jpg" alt="img"></p>
<p><img src="v2-f268035371aa22a350a317fc237a04f7_720w.jpg" alt="img"></p>
<p>既然self-attention是更广义的CNN，则这个模型更加flexible。而我们认为，一个模型越flexible，训练它所需要的数据量就越多，所以在训练self-attention模型时就需要更多的数据，论文 ViT 中有印证，它需要的数据集是有3亿张图片的JFT-300，而如果不使用这么多数据而只使用ImageNet，则性能不如CNN。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/06/Swin-Transformer%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="篮球架上打砖块">
      <meta itemprop="description" content="Apodidae">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Upperlan">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/06/Swin-Transformer%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">Swin Transformer笔记-Shifted机制与MSA机制</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-06 16:12:19" itemprop="dateCreated datePublished" datetime="2022-03-06T16:12:19+08:00">2022-03-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-03-07 11:07:30" itemprop="dateModified" datetime="2022-03-07T11:07:30+08:00">2022-03-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94%E4%B8%8E%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">科研与论文</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>原文来自：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/367111046">https://zhuanlan.zhihu.com/p/367111046</a></p>
<p>我加上一些代码的注释方便阅读</p>
<h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p><img src="9da511d843648230e72d5dc78e58a148-6556114.png" alt="9da511d843648230e72d5dc78e58a148"></p>
<p>一共包含4个Stage，每个stage都会缩小输入特征图的分辨率，像CNN一样逐层扩大感受野。框架代码大致如下，省略了一些参数与层的细节。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SwinTransformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">...</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># absolute position embedding</span></span><br><span class="line">        <span class="keyword">if</span> self.ape: <span class="comment"># 是否加入位置编码</span></span><br><span class="line">            self.absolute_pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches, embed_dim))</span><br><span class="line">            <span class="comment"># 每个head的shape都是num_patches * embed_dim</span></span><br><span class="line">        self.pos_drop = nn.Dropout(p=drop_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># build layers</span></span><br><span class="line">        self.layers = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i_layer <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            layer = BasicLayer(...)</span><br><span class="line">            self.layers.append(layer)</span><br><span class="line"></span><br><span class="line">        self.norm = norm_layer(self.num_features)</span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool1d(<span class="number">1</span>)</span><br><span class="line">        self.head = nn.Linear(self.num_features, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.patch_embed(x) <span class="comment"># 先取得图象块的embeddings</span></span><br><span class="line">        <span class="keyword">if</span> self.ape: <span class="comment"># 位置编码是否需要</span></span><br><span class="line">            x = x + self.absolute_pos_embed</span><br><span class="line">        x = self.pos_drop(x) <span class="comment"># dropout层</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x)</span><br><span class="line"></span><br><span class="line">        x = self.norm(x)  <span class="comment"># B L C</span></span><br><span class="line">        x = self.avgpool(x.transpose(<span class="number">1</span>, <span class="number">2</span>))  <span class="comment"># B C L</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>) <span class="comment"># 展平</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.forward_features(x)</span><br><span class="line">        x = self.head(x) <span class="comment"># 线性层</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>其中有几个地方处理方法与ViT不同：</p>
<ul>
<li>ViT在输入会给embedding进行位置编码。而Swin-T这里则是作为一个<strong>可选项</strong>（<code>self.ape</code>），Swin-T是在计算Attention的时候做了一个<code>相对位置编码</code></li>
<li>ViT会单独加上一个可学习参数，作为分类的token。而Swin-T则是<strong>直接做平均</strong>，输出分类，有点类似CNN最后的全局平均池化层</li>
</ul>
<p>接下来我们看下各个组件的构成</p>
<h2 id="Patch-Embedding"><a href="#Patch-Embedding" class="headerlink" title="Patch Embedding"></a><strong>Patch Embedding</strong></h2><p>在输入进Block前，我们需要将图片切成一个个patch，然后嵌入向量。</p>
<p>具体做法是对原始图片裁成一个个 <code>patch_size * patch_size</code>的窗口大小，然后进行嵌入。</p>
<p>这里可以通过二维卷积层，<strong>将stride，kernelsize设置为patch_size大小</strong>。设定输出通道来确定嵌入向量的大小。最后将H,W维度展开，并移动到第一维度，这里的处理与ViT是一样的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">4</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">96</span>, norm_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        img_size = to_2tuple(img_size) <span class="comment"># -&gt;  将int类型的size转换成tuple类型数据:(img_size, img_size)</span></span><br><span class="line">        patch_size = to_2tuple(patch_size) <span class="comment"># -&gt; (patch_size, patch_size)</span></span><br><span class="line">        patches_resolution = [img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]]<span class="comment"># patch的行，列数</span></span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.patches_resolution = patches_resolution</span><br><span class="line">        self.num_patches = patches_resolution[<span class="number">0</span>] * patches_resolution[<span class="number">1</span>] <span class="comment"># 一共多少个patches</span></span><br><span class="line"></span><br><span class="line">        self.in_chans = in_chans <span class="comment"># 输入通道数</span></span><br><span class="line">        self.embed_dim = embed_dim <span class="comment"># 一个嵌入向量的维度，即一个patch的数据展平后通过嵌入层线性映射的目标向量维度</span></span><br><span class="line"></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) <span class="comment"># 逐行逐个块取线性映射</span></span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># layer norm层</span></span><br><span class="line">            self.norm = norm_layer(embed_dim)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.norm = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 假设采取默认参数</span></span><br><span class="line">        x = self.proj(x) <span class="comment"># 出来的是(N, 96, 224/4, 224/4) </span></span><br><span class="line">        x = torch.flatten(x, <span class="number">2</span>) <span class="comment"># 把HW维展开，(N, 96, 56*56)</span></span><br><span class="line">        x = torch.transpose(x, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 把通道维放到最后 (N, 56*56, 96)</span></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="Patch-Merging"><a href="#Patch-Merging" class="headerlink" title="Patch Merging"></a><strong>Patch Merging</strong></h2><p>该模块的作用是在每个Stage开始前做降采样，用于缩小分辨率，调整通道数 进而形成层次化的设计，同时也能节省一定运算量。</p>
<blockquote>
<p>在CNN中，则是在每个Stage开始前用<code>stride=2</code>的卷积/池化层来降低分辨率。</p>
</blockquote>
<p>每次降采样是两倍，因此<strong>在行方向和列方向上，间隔2选取元素</strong>。</p>
<p>然后拼接在一起作为一整个张量，最后展开。<strong>此时通道维度会变成原先的4倍</strong>（因为H,W各缩小2倍），此时再通过一个<strong>全连接层再调整通道维度为原来的两倍</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerging</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_resolution, dim, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.input_resolution = input_resolution</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.reduction = nn.Linear(<span class="number">4</span> * dim, <span class="number">2</span> * dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.norm = norm_layer(<span class="number">4</span> * dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: B, H*W, C</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        H, W = self.input_resolution</span><br><span class="line">        B, L, C = x.shape <span class="comment"># L就等于num_patches</span></span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> H % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> W % <span class="number">2</span> == <span class="number">0</span>, <span class="string">f&quot;x size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) are not even.&quot;</span></span><br><span class="line"></span><br><span class="line">        x = x.view(B, H, W, C) <span class="comment"># 将特征图再分成按patch为单位，长为W，高为H的形状排列</span></span><br><span class="line">				<span class="comment"># 对原图进行取样，降低image_size</span></span><br><span class="line">        x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], -<span class="number">1</span>)  <span class="comment"># B H/2 W/2 4*C</span></span><br><span class="line">        x = x.view(B, -<span class="number">1</span>, <span class="number">4</span> * C)  <span class="comment"># B H/2*W/2 4*C size是原来的二分之一，通道数是原来的2倍</span></span><br><span class="line"></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.reduction(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>下面是一个示意图（输入张量N=1, H=W=8, C=1，不包含最后的全连接层调整）</p>
<p><img src="f26297bb391b288a896c6519b141a19f-6559133.png" alt="f26297bb391b288a896c6519b141a19f"></p>
<p><img src="v2-f9c4e3d69da7508562358f9c3f683c63_1440w.png" alt="img"></p>
<blockquote>
<p>个人感觉这像是PixelShuffle的反操作</p>
</blockquote>
<h2 id="Window-Partition-Reverse"><a href="#Window-Partition-Reverse" class="headerlink" title="Window Partition/Reverse"></a><strong>Window Partition/Reverse</strong></h2><p><code>window partition</code>函数是用于对张量划分窗口，指定窗口大小。将原本的张量从 <code>N H W C</code>, 划分成 <code>num_windows*B, window_size, window_size, C</code>，其中 <code>num_windows = H*W / window_size</code>，即窗口的个数，这里的窗口是由patches为单位组成的。而<code>window reverse</code>函数则是对应的逆过程。这两个函数会在后面的<code>Window Attention</code>用到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">window_partition</span>(<span class="params">x, window_size</span>):</span><br><span class="line">    B, H, W, C = x.shape</span><br><span class="line">    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)</span><br><span class="line">    windows = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(-<span class="number">1</span>, window_size, window_size, C)</span><br><span class="line">    <span class="keyword">return</span> windows <span class="comment"># (num_windows*B, window_size, window_size, C)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">window_reverse</span>(<span class="params">windows, window_size, H, W</span>):</span><br><span class="line">    B = <span class="built_in">int</span>(windows.shape[<span class="number">0</span>] / (H * W / window_size / window_size))</span><br><span class="line">    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -<span class="number">1</span>)</span><br><span class="line">    x = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(B, H, W, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="Window-Attention"><a href="#Window-Attention" class="headerlink" title="Window Attention"></a><strong>Window Attention</strong></h2><p>这是这篇文章的关键。传统的Transformer都是<strong>基于全局来计算注意力的</strong>，因此计算复杂度十分高。而Swin Transformer则将<strong>注意力的计算限制在每个窗口内</strong>，进而减少了计算量。</p>
<p>我们先简单看下公式</p>
<script type="math/tex; mode=display">
Attention\left ( Q,K,T\right )=Softmax\left ( \frac{QK^{T}}{\sqrt{d}+B}\right )V</script><p><img src="eaff53eb1815f6e3c84d3a7b36783701.png" alt="eaff53eb1815f6e3c84d3a7b36783701"></p>
<p>主要区别是在原始计算Attention的公式中的Q,K时<strong>加入了相对位置编码</strong>。后续实验有证明相对位置编码的加入提升了模型性能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">WindowAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias.</span></span><br><span class="line"><span class="string">    It supports both of shifted and non-shifted window.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        window_size (tuple[int]): The height and width of the window.(int, int)</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0</span></span><br><span class="line"><span class="string">        proj_drop (float, optional): Dropout ratio of output. Default: 0.0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, window_size, num_heads, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.window_size = window_size  <span class="comment"># Wh, Ww 和前面的patch size区分</span></span><br><span class="line">        self.num_heads = num_heads <span class="comment"># nH</span></span><br><span class="line">        head_dim = dim // num_heads <span class="comment"># 每个注意力头对应的通道数</span></span><br><span class="line">        self.scale = qk_scale <span class="keyword">or</span> head_dim ** -<span class="number">0.5</span> <span class="comment"># (前面公式的根号d)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># define a parameter table of relative position bias</span></span><br><span class="line">        self.relative_position_bias_table = nn.Parameter(</span><br><span class="line">            torch.zeros((<span class="number">2</span> * window_size[<span class="number">0</span>] - <span class="number">1</span>) * (<span class="number">2</span> * window_size[<span class="number">1</span>] - <span class="number">1</span>), num_heads))  </span><br><span class="line">        <span class="comment"># 设置一个形状为（2*(Wh-1) * 2*(Ww-1), nH）的可学习变量，用于后续的位置编码</span></span><br><span class="line">  </span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias) <span class="comment"># qkv三个向量的计算综合到一起</span></span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">        trunc_normal_(self.relative_position_bias_table, std=<span class="number">.02</span>)</span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line">     <span class="comment"># 相关位置编码...</span></span><br></pre></td></tr></table></figure>
<p>下面把涉及到相关位置编码的逻辑给单独拿出来，这部分比较绕</p>
<p>首先QK计算出来的Attention张量形状为<code>(numWindows*B, num_heads, window_size*window_size, window_size*window_size)</code>。</p>
<p>我们利用<code>torch.arange</code>和<code>torch.meshgrid</code>函数生成对应的坐标，这里我们以<code>windowsize=2</code>为例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">coords_h = torch.arange(self.window_size[<span class="number">0</span>])</span><br><span class="line">coords_w = torch.arange(self.window_size[<span class="number">1</span>])</span><br><span class="line">coords = torch.meshgrid([coords_h, coords_w]) <span class="comment"># -&gt; 2*(wh, ww)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  (tensor([[0, 0],</span></span><br><span class="line"><span class="string">           [1, 1]]), </span></span><br><span class="line"><span class="string">   tensor([[0, 1],</span></span><br><span class="line"><span class="string">           [0, 1]]))</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>然后堆叠起来，展开为一个二维向量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">coords = torch.stack(coords)  <span class="comment"># 2, Wh, Ww</span></span><br><span class="line">coords_flatten = torch.flatten(coords, <span class="number">1</span>)  <span class="comment"># 2, Wh*Ww</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[0, 0, 1, 1],</span></span><br><span class="line"><span class="string">        [0, 1, 0, 1]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>利用广播（broadcast）机制，分别在第一维，第二维，插入一个维度，进行广播相减，得到 <code>2, wh*ww, wh*ww</code>的张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">relative_coords_first = coords_flatten[:, :, <span class="literal">None</span>]  <span class="comment"># 2, wh*ww, 1</span></span><br><span class="line">relative_coords_second = coords_flatten[:, <span class="literal">None</span>, :] <span class="comment"># 2, 1, wh*ww</span></span><br><span class="line">relative_coords = relative_coords_first - relative_coords_second <span class="comment"># 最终得到 2, wh*ww, wh*ww 形状的张量</span></span><br></pre></td></tr></table></figure>
<p>因为采取的是相减，所以得到的索引是从负数开始的，<strong>我们加上偏移量，让其从0开始</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">relative_coords = relative_coords.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).contiguous() <span class="comment"># Wh*Ww, Wh*Ww, 2</span></span><br><span class="line">relative_coords[:, :, <span class="number">0</span>] += self.window_size[<span class="number">0</span>] - <span class="number">1</span></span><br><span class="line">relative_coords[:, :, <span class="number">1</span>] += self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>后续我们需要将其展开成一维偏移量。而对于(1，2）和（2，1）这两个坐标。在二维上是不同的，<strong>但是通过将x,y坐标相加转换为一维偏移的时候，他的偏移量是相等的</strong>。</p>
<p><img src="v2-5b1f589ca71a4bc406a266296025b4b4_1440w.jpg" alt="img"></p>
<p>所以最后我们对其中做了个乘法操作，以进行区分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">relative_coords[:, :, <span class="number">0</span>] *= <span class="number">2</span> * self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p><img src="v2-0c99206fb39da67bae3415a650c38742_1440w.png" alt="img"></p>
<p>然后再最后一维上进行求和，展开成一个一维坐标，并注册为一个不参与网络学习的变量</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww</span><br><span class="line">self.register_buffer(&quot;relative_position_index&quot;, relative_position_index)</span><br></pre></td></tr></table></figure>
<p>接着我们看前向代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x: input features with shape of (num_windows*B, N, C)</span></span><br><span class="line"><span class="string">        mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    B_, N, C = x.shape</span><br><span class="line">    </span><br><span class="line">    qkv = self.qkv(x).reshape(B_, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">    q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]  <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">    q = q * self.scale</span><br><span class="line">    attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-<span class="number">1</span>)].view(</span><br><span class="line">        self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], -<span class="number">1</span>)  <span class="comment"># Wh*Ww,Wh*Ww,nH</span></span><br><span class="line">    relative_position_bias = relative_position_bias.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).contiguous()  <span class="comment"># nH, Wh*Ww, Wh*Ww</span></span><br><span class="line">    attn = attn + relative_position_bias.unsqueeze(<span class="number">0</span>) <span class="comment"># (1, num_heads, windowsize, windowsize)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># 下文会分析到</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        attn = self.softmax(attn)</span><br><span class="line"></span><br><span class="line">    attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">    x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B_, N, C)</span><br><span class="line">    x = self.proj(x)</span><br><span class="line">    x = self.proj_drop(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li>首先输入张量形状为 <code>numWindows*B, window_size * window_size, C</code>（后续会解释）</li>
<li>然后经过<code>self.qkv</code>这个全连接层后，进行reshape，调整轴的顺序，得到形状为<code>3, numWindows*B, num_heads, window_size*window_size, c//num_heads</code>，并分配给<code>q,k,v</code>。</li>
<li>根据公式，我们对<code>q</code>乘以一个<code>scale</code>缩放系数，然后与<code>k</code>（为了满足矩阵乘要求，需要将最后两个维度调换）进行相乘。得到形状为<code>(numWindows*B, num_heads, window_size*window_size, window_size*window_size)</code>的<code>attn</code>张量</li>
<li>之前我们针对位置编码设置了个形状为<code>(2*window_size-1*2*window_size-1, numHeads)</code>的可学习变量。我们用计算得到的相对编码位置索引<code>self.relative_position_index</code>选取，得到形状为<code>(window_size*window_size, window_size*window_size, numHeads)</code>的编码，加到<code>attn</code>张量上</li>
<li>暂不考虑mask的情况，剩下就是跟transformer一样的softmax，dropout，与<code>V</code>矩阵乘，再经过一层全连接层和dropout</li>
</ul>
<h2 id="Shifted-Window-Attention"><a href="#Shifted-Window-Attention" class="headerlink" title="Shifted Window Attention"></a><strong>Shifted Window Attention</strong></h2><p>前面的Window Attention是在每个窗口下计算注意力的，为了更好的和其他window进行信息交互，Swin Transformer还引入了shifted window操作。</p>
<p><img src="v2-07a98325a29db1da6521e4ddaaed3c88_1440w.jpg" alt="img"></p>
<p>左边是没有重叠的Window Attention，而右边则是将窗口进行移位的Shift Window Attention。可以看到移位后的窗口包含了原本相邻窗口的元素。但这也引入了一个新问题，即<strong>window的个数翻倍了</strong>，由原本四个窗口变成了9个窗口。</p>
<p>在实际代码里，我们是<strong>通过对特征图移位，并给Attention设置mask来间接实现的</strong>。能在<strong>保持原有的window个数下</strong>，最后的计算结果等价。</p>
<p><img src="v2-84b7dd5ba83bf0c686a133dec758d974_1440w.jpg" alt="img"></p>
<h2 id="特征图移位操作"><a href="#特征图移位操作" class="headerlink" title="特征图移位操作"></a><strong>特征图移位操作</strong></h2><p>代码里对特征图移位是通过<code>torch.roll</code>来实现的，下面是示意图</p>
<p><img src="v2-7b594ca54a3cfac5370d8fef2be6f768_1440w.jpg" alt="img"></p>
<blockquote>
<p>如果需要<code>reverse cyclic shift</code>的话只需把参数<code>shifts</code>设置为对应的正数值。</p>
</blockquote>
<h2 id="Attention-Mask"><a href="#Attention-Mask" class="headerlink" title="Attention Mask"></a><strong>Attention Mask</strong></h2><p>我认为这是Swin Transformer的精华，通过设置合理的mask，让<code>Shifted Window Attention</code>在与<code>Window Attention</code>相同的窗口个数下，达到等价的计算结果。</p>
<p>首先我们对Shift Window后的每个窗口都给上index，并且做一个<code>roll</code>操作（window_size=2, shift_size=-1）</p>
<p><img src="v2-d80364e0b73c60bcd8a60bbd91cfbaeb_1440w.jpg" alt="img"></p>
<p>我们希望在计算Attention的时候，<strong>让具有相同index QK进行计算，而忽略不同index QK计算结果</strong>。</p>
<p>最后正确的结果如下图所示</p>
<p><img src="v2-e72bf67b5cbcc27e2d2640bcd3522d0e_1440w.jpg" alt="img"></p>
<p>而要想在原始四个窗口下得到正确的结果，我们就必须给Attention的结果加入一个mask（如上图最右边所示）</p>
<p>相关代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># calculate attention mask for SW-MSA</span></span><br><span class="line">    H, W = self.input_resolution</span><br><span class="line">    img_mask = torch.zeros((<span class="number">1</span>, H, W, <span class="number">1</span>))  <span class="comment"># 1 H W 1</span></span><br><span class="line">    h_slices = (<span class="built_in">slice</span>(<span class="number">0</span>, -self.window_size),</span><br><span class="line">                <span class="built_in">slice</span>(-self.window_size, -self.shift_size),</span><br><span class="line">                <span class="built_in">slice</span>(-self.shift_size, <span class="literal">None</span>))</span><br><span class="line">    w_slices = (<span class="built_in">slice</span>(<span class="number">0</span>, -self.window_size),</span><br><span class="line">                <span class="built_in">slice</span>(-self.window_size, -self.shift_size),</span><br><span class="line">                <span class="built_in">slice</span>(-self.shift_size, <span class="literal">None</span>))</span><br><span class="line">    cnt = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> h_slices:</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> w_slices:</span><br><span class="line">            img_mask[:, h, w, :] = cnt</span><br><span class="line">            cnt += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    mask_windows = window_partition(img_mask, self.window_size)  <span class="comment"># nW, window_size, window_size, 1</span></span><br><span class="line">    mask_windows = mask_windows.view(-<span class="number">1</span>, self.window_size * self.window_size)</span><br><span class="line">    attn_mask = mask_windows.unsqueeze(<span class="number">1</span>) - mask_windows.unsqueeze(<span class="number">2</span>)</span><br><span class="line">    attn_mask = attn_mask.masked_fill(attn_mask != <span class="number">0</span>, <span class="built_in">float</span>(-<span class="number">100.0</span>)).masked_fill(attn_mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="number">0.0</span>))</span><br></pre></td></tr></table></figure>
<p>以上图的设置，我们用这段代码会得到这样的一个mask</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[[   0.,    0.,    0.,    0.],</span><br><span class="line">           [   0.,    0.,    0.,    0.],</span><br><span class="line">           [   0.,    0.,    0.,    0.],</span><br><span class="line">           [   0.,    0.,    0.,    0.]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">         [[[   0., -100.,    0., -100.],</span><br><span class="line">           [-100.,    0., -100.,    0.],</span><br><span class="line">           [   0., -100.,    0., -100.],</span><br><span class="line">           [-100.,    0., -100.,    0.]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">         [[[   0.,    0., -100., -100.],</span><br><span class="line">           [   0.,    0., -100., -100.],</span><br><span class="line">           [-100., -100.,    0.,    0.],</span><br><span class="line">           [-100., -100.,    0.,    0.]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">         [[[   0., -100., -100., -100.],</span><br><span class="line">           [-100.,    0., -100., -100.],</span><br><span class="line">           [-100., -100.,    0., -100.],</span><br><span class="line">           [-100., -100., -100.,    0.]]]]])</span><br></pre></td></tr></table></figure>
<p>在之前的window attention模块的前向代码里，包含这么一段</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    nW = mask.shape[<span class="number">0</span>]</span><br><span class="line">    attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    attn = attn.view(-<span class="number">1</span>, self.num_heads, N, N)</span><br><span class="line">    attn = self.softmax(attn)</span><br></pre></td></tr></table></figure>
<p>将mask加到attention的计算结果，并进行softmax。mask的值设置为-100，softmax后就会忽略掉对应的值</p>
<h2 id="Transformer-Block整体架构"><a href="#Transformer-Block整体架构" class="headerlink" title="Transformer Block整体架构"></a><strong>Transformer Block整体架构</strong></h2><p><img src="v2-b1f64ea254af2c7b1cdbaf9288731371_1440w.jpg" alt="img"></p>
<p>两个连续的Block架构如上图所示，需要注意的是一个Stage包含的Block个数必须是偶数，因为需要交替包含一个含有<code>Window Attention</code>的Layer和含有<code>Shifted Window Attention</code>的Layer。</p>
<p>我们看下Block的前向代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    H, W = self.input_resolution</span><br><span class="line">    B, L, C = x.shape</span><br><span class="line">    <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line"></span><br><span class="line">    shortcut = x</span><br><span class="line">    x = self.norm1(x)</span><br><span class="line">    x = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># cyclic shift</span></span><br><span class="line">    <span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">        shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shifted_x = x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># partition windows</span></span><br><span class="line">    x_windows = window_partition(shifted_x, self.window_size)  <span class="comment"># nW*B, window_size, window_size, C</span></span><br><span class="line">    x_windows = x_windows.view(-<span class="number">1</span>, self.window_size * self.window_size, C)  <span class="comment"># nW*B, window_size*window_size, C</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># W-MSA/SW-MSA</span></span><br><span class="line">    attn_windows = self.attn(x_windows, mask=self.attn_mask)  <span class="comment"># nW*B, window_size*window_size, C</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># merge windows</span></span><br><span class="line">    attn_windows = attn_windows.view(-<span class="number">1</span>, self.window_size, self.window_size, C)</span><br><span class="line">    shifted_x = window_reverse(attn_windows, self.window_size, H, W)  <span class="comment"># B H&#x27; W&#x27; C</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># reverse cyclic shift</span></span><br><span class="line">    <span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">        x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = shifted_x</span><br><span class="line">    x = x.view(B, H * W, C)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># FFN</span></span><br><span class="line">    x = shortcut + self.drop_path(x)</span><br><span class="line">    x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>整体流程如下</p>
<ul>
<li>先对特征图进行LayerNorm</li>
<li>通过<code>self.shift_size</code>决定是否需要对特征图进行shift</li>
<li>然后将特征图切成一个个窗口</li>
<li>计算Attention，通过<code>self.attn_mask</code>来区分<code>Window Attention</code>还是<code>Shift Window Attention</code></li>
<li>将各个窗口合并回来</li>
<li>如果之前有做shift操作，此时进行<code>reverse shift</code>，把之前的shift操作恢复</li>
<li>做dropout和残差连接</li>
<li>再通过一层LayerNorm+全连接层，以及dropout和残差连接</li>
</ul>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><strong>实验结果</strong></h2><p><img src="v2-bf00e048de979decd68ebc7c5372cb27_1440w.jpg" alt="img"></p>
<p>在ImageNet22K数据集上，准确率能达到惊人的86.4%。另外在检测，分割等任务上表现也很优异，感兴趣的可以翻看论文最后的实验部分。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>这篇文章创新点很棒，引入window这一个概念，将CNN的局部性引入，还能控制模型整体计算量。在Shift Window Attention部分，用一个mask和移位操作，很巧妙的实现计算等价。作者的代码也写得十分赏心悦目，推荐阅读！</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/06/%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87%E4%B9%A6%E5%86%99%E5%B0%8F%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="篮球架上打砖块">
      <meta itemprop="description" content="Apodidae">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Upperlan">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/06/%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87%E4%B9%A6%E5%86%99%E5%B0%8F%E8%AE%B0/" class="post-title-link" itemprop="url">科研论文书写小记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-06 13:28:37" itemprop="dateCreated datePublished" datetime="2022-03-06T13:28:37+08:00">2022-03-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-03-10 20:02:12" itemprop="dateModified" datetime="2022-03-10T20:02:12+08:00">2022-03-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94%E4%B8%8E%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">科研与论文</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>来源：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/368677897">https://zhuanlan.zhihu.com/p/368677897</a></p>
<p><img src="v2-37eb4b8e53a842758725be0936c9a07e_720w.jpg" alt="img"></p>
<p>（一种很典型的行文方式：把所有工作摊煎饼地摆出来，反正我列出来了，哪个有用你自己找)</p>
<p>“做这个工作的有ABCD, A 做了xxx，B做了xxx，C和D做了xxx” ——类似的句式，<strong>不要再这样写啦</strong>。写相关工作的时候，作者应该是充当一种指路人，或者说书人的角色：在详细介绍核心内容之前，<strong>把故事背景在读者面前慢慢地，有条理地展开</strong>。</p>
<p>所以，你需要把面前的相关文献们，用一条线串起来，来向读者展现它们之间，以及和自己工作之间的关联。这些逻辑可能是递进的：<strong>例如文章ABC开创了某个领域，DEF在这个问题领域提出了集中思路，GHI则是在各个思路下的改进——那么我们就可以先说用总领的话来叙述ABC，然后用并列的方式讲DEF，最后可以一笔带过GHI</strong>（如果它们不是那么重要的话）。</p>
<p>这些逻辑可能是并列的：例如文章ABC用了X技术来解决某个问题，DEF用了Y技术，而GHI则用了DrustZ的技术——那么我们可以使用分条列举的方式，先写”为了解决某个问题，大家采用了不同的方法“，然后列出1）X技术[ABC] 2）Y技术[DEF] 3) DrustZ的技术[GHI] 。这样把类似的文章集中起来，大家看着也顺溜。</p>
<p>还有些其他不怎么常见的逻辑顺序，比如分-总（例如先有零散的工作，后来有一篇文献把它们综合了起来）。但是当我们意识到文献内在的逻辑顺序，并且把它们按照逻辑列举出来的时候，就已经比许多论文的相关文献高出了不止一个档次：）</p>
<p>更加高级的写作方式，则是与其列举文献，更像是让文献为作者自己的陈述服务。你在阅读的过程中，很难感觉到作者在刻意塞进一些文献来充数；相反，作者是在整理自己的思路，顺便引用了关键的文献来加以佐证。</p>
<p>开头不要直接列别人的文章，先概括地说一下这个段落的主题；写完一个小部分，不要立即跳到下一段开始写另一个部分，而是在末尾写一个过渡句：“以上就是人们在X领域的研究，但是随着Y技术的进步，更多的人把目光转移到了Z上”，“对于Y技术在X领域的研究已经介绍许多，但在另一个Z方面，相同的技术也有许多应用”。拿出小学作文满分的本领来！</p>
<p>我老板写Related Work有一个习惯，从来不会在大标题下什么都不写就直接跳到第一个小标题（例如上边那个反例）。在大标题的下面，用一些总领的语言来概括+引出接下来的小章节，其实也是一个很好的过渡方式。一切为了读者服务！</p>
<p><img src="v2-cafd63855ff2e5659ed60614d4280d62_720w.jpg" alt="img"></p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>作者：平海鸥鸣<br>链接：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/406397855">https://zhuanlan.zhihu.com/p/406397855</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<p><strong>将研究成果和研究目的联系起来</strong><br>Simple statistical analysis was used to …<br>The next question asked the informants …<br>To assess X, the Y questionnaire was used.<br>Changes in X and Y were compared using …<br>Regression analysis was used to predict the …<br>To distinguish between these two possibilities, …<br>The first set of analyses examined the impact of … </p>
<p><strong>描述图标中的研究成果</strong><br>As shown in Figure 1,<br>As can be seen from the table (above),<br>From the graph above we can see that<br>It can be seen from the data in Table 1 that </p>
<p>It is apparent from this table that very few …<br>The most interesting aspect of this graph is …<br>In Fig.10 there is a clear trend of decreasing …<br>What is striking about the figures in this table is …<br>What is interesting about the data in this table is that …<br>The differences between X and Y are highlighted in Table 4.<br>From the chart, it can be seen that by far the greatest demand is for …<br>From this data, we can see that Study 2 resulted in the lowest value of … </p>
<p><strong>只是描述研究结果</strong><br><strong>正面的</strong><br>The mean score for X was …<br>Further analysis showed that …<br>Further statistical tests revealed …<br>A two-way ANOVA revealed that …<br>On average, Xs were shown to have …<br>Strong evidence of X was found when …<br>This result is significant at the p = 0.05 level.<br>The results, as shown in Table 1, indicate that …<br>A positive correlation was found between X and Y.<br>There was a significant positive correlation between … </p>
<p><strong>负面的</strong><br>No difference greater than X was observed.<br>No significant differences were found between …<br>None of these differences were statistically significant.<br>No significant difference between the two groups was evident.<br>No significant reduction in X was found compared with placebo.<br>No evidence was found for non-linear associations between X and Y.<br>No significant correlation was found between X scores and the Y scores (p = .274) </p>
<p><strong>将之前的结果（正面的或负面的）——进行简单的现象描述</strong><br>Stimulation of X cells with Y did not increase the …<br>With successive increases in intensity of the X, the Y moved further to …<br>Following the addition of X, a significant increase (<em>P</em>&lt;0.05) in the Y was recorded.<br>When X cells were stimulated with Y, no significant difference in the number of Z was detected. </p>
<p><strong>如果发现有意思的结果</strong></p>
<p>This result is somewhat counterintuitive.<br>Interestingly, this correlation is related to …<br>The more surprising correlation is with the …<br>Surprisingly, only a minority of respondents …<br>The most surprising aspect of the data is in the …<br>The correlation between X and Y is interesting because …<br>The most striking result to emerge from the data is that …<br>Interestingly, there were also differences in the ratios of …<br>The single most striking observation to emerge from the data comparison was … </p>
<p><strong>然后，就开始说我们的对自己研究结果的解释</strong><br>This result may be explained by the fact that …<br>There are, however, other possible explanations.<br>These relationships may partly be explained by …<br>There are several possible explanations for this result.<br>A possible explanation for these results may be the lack of adequate …<br>These differences can be explained in part by the proximity of X and Y. </p>
<p><strong>描述另一个实验结果</strong></p>
<p>A comparison of the two results reveals …<br>Turning now to the experimental evidence on …<br>Comparing the two results, it can be seen that …<br>The next section of the survey was concerned with …<br>In the final part of the survey, respondents were asked … </p>
<p><strong>最后肯定就是对现有研究工作进行的总结</strong></p>
<p>Overall, these results indicate that …<br>In summary, these results show that …<br>In summary, for the informants in this study, …<br>Together these results provide important insights into …<br>Taken together, these results suggest that there is an association between …<br>The results in this chapter indicate that … The next chapter, therefore, moves on to discuss the …</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/06/ATSS%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Anchor-based%E5%92%8CAnchor-free%E6%A8%A1%E5%9E%8B%E5%85%B3%E9%94%AE%E5%B7%AE%E5%BC%82/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="篮球架上打砖块">
      <meta itemprop="description" content="Apodidae">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Upperlan">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/06/ATSS%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Anchor-based%E5%92%8CAnchor-free%E6%A8%A1%E5%9E%8B%E5%85%B3%E9%94%AE%E5%B7%AE%E5%BC%82/" class="post-title-link" itemprop="url">ATSS论文笔记:Anchor-based和Anchor-free模型关键差异</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-03-06 11:42:13 / 修改时间：11:58:22" itemprop="dateCreated datePublished" datetime="2022-03-06T11:42:13+08:00">2022-03-06</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">目标检测</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>论文: Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</p>
<p>简述：论文指出one-stage anchor-based和center-based anchor-free检测算法间的差异主要来自于正负样本的选择，基于此提出ATSS(Adaptive Training Sample Selection)方法，该方法能够<strong>自动根据GT的相关统计特征选择合适的anchor box作为正样本</strong>，在<strong>不带来额外计算量和参数</strong>的情况下，能够大幅提升模型的性能，十分有用。</p>
<p>背景：RetinaNet是一个anchor-based检测模型，FCOS是一个anchor-free检测模型，两者有三点不同：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>RetinaNet</th>
<th>FCOS</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.每个位置的anchor数</td>
<td>RetinaNet每个grid cell有几个anchor box</td>
<td>FCOS每个位置只根据一个anchor point来回归bbox</td>
</tr>
<tr>
<td>2.正负样本的定义不同</td>
<td>根据IoU来区分正副样本</td>
<td>根据选择样本的空间和规模限制（即gt框中心周围的一部分区域的点作为正样本。）</td>
</tr>
<tr>
<td>3.回归的对象不同</td>
<td>回归anchor box的偏移值</td>
<td>回归点到bbox四条边的距离</td>
</tr>
</tbody>
</table>
</div>
<p>实验中FCOS比RetinaNet的效果要好</p>
<p>实验：论文选取<strong>anchor-based方法RetinaNet</strong>和<strong>anchor-free方法FCOS</strong>进行对比，主要对比正负样本定义和回归开始状态的差异，将RetinaNet的anchor数改为1降低差异性，方便与FCOS比较，后续会测试anchor数带来的作用。</p>
<p>首先将FCOS上有的tricks加在RetinaNet上</p>
<p><img src="image-20220306115011025-6538616.png" alt="image-20220306115011025"></p>
<p>现在FCOS和RetinaNet只有两个不同：</p>
<p>1.正负样本的分配</p>
<p>2.回归对象的不同</p>
<p>而此时FCOS比RetinaNet高0.8个点，所以再验证这两个不同的影响：</p>
<p><img src="image2022-2-11_16-10-11.png" alt="image2022-2-11_16-10-11"></p>
<p>又上图得RetinaNet将IoU阈值分配正负样本（与yolov3相同）换成点和区域分配正负样本时，涨点了，而FCOS使用IoU阈值时反而掉点了</p>
<p>后续实验验证回归对象的时候发现对象不同几乎不影响mAP。</p>
<p>因此作者得出结论：</p>
<p>FCOS的表现比RetinaNet要好主要是因为FCOS的正负样本分配策略更好。</p>
<p>因此对于anchor-based模型，为了消除anchor超参数的影响，提出</p>
<p><strong>Adaptive Training Sample Selection (ATSS)</strong></p>
<p>自动根据物体的统计特征划分正负样本，几乎没有任何超参数。</p>
<p><img src="image-20220306115249710-6538771.png" alt="image-20220306115249710"></p>
<p>先选出每一层的所有预测框中与真实框L2距离最小的k个预测框，计算这些预测框与每一个gt框的IoU，统计这些IoU值的均值m和均方差v，则IoU阈值t=m+v。如果一个预测框与某个gt框的阈值大于t，则为正样本，否则为负样本。</p>
<p>m：为了有合适数目的正样本，如果m很大，说明有许多高质量的预测框，则IoU阈值应该更大；如果m比较小，说明预测框的质量不太好，IoU阈值应该更小</p>
<p>v：如果v较小，说明有几个特征层都适合完成当前的检测任务，所以m+v的值更小，则选取更多正样本；如果v较大，则说明有某些特定的特征层更适合完成当前目标的检测任务，所以IoU阈值更大，来选取更合适的特征层上的检测框</p>
<p>根据统计学理论，IoU值在阈值t以上的预测框大概在16%左右，每个目标物体大概会分配 （0.2 <em> k </em> 特征层数 ）个正样本。该方法只引入了一个超参数k，而且k对结果的影响并不大，一般设置为9，因此本算法可以近似看做不增加超参数。</p>
<p><img src="image-20220306115325575-6538806.png" alt="image-20220306115325575"></p>
<ul>
<li>将RetinaNet中的正负样本替换为ATSS，AP提升了2.9%，这样的性能提升几乎是没有任何额外消耗的</li>
<li>在FCOS上的应用主要用两种：lite版本采用ATSS的思想，从选取GT内的anchor point改为选取每层离GT最近的top $k$个候选anchor point，提升了0.8%AP；full版本将FCOS的anchor point改为长宽为$8S$的anchor box来根据ATSS选择正负样本，但仍然使用原始的回归方法，提升了1.4%AP。两种方法找到的anchor point在空间位置上大致相同，但是在FPN层上的选择不太一样。从结果来看，自适应的选择方法比固定的方法更有效</li>
</ul>
<p>这里的RetinaNet只用了一个anchor box，如果使用多个anchor box，结果如下：</p>
<p><img src="image-20220306115542160-6538944.png" alt="image-20220306115542160"></p>
<p>Imprs为用在了FCOS中的提升手段。从结果来看，在每个位置设定多个anchor box是无用的操作，关键在于选择合适的正样本</p>
<p>论文结论：</p>
<p>1.这两种方法之间的本质区别是<strong>正负训练样本的选择</strong>，这导致了它们之间的性能差距。如果他们在训练期间选择相同的正负样本，无论是回归anchor bbox或anchor point，最终表现都没有明显的差距。</p>
<p>2.通过一系列实验，可以得出结论，没有必要在图像上每个位置设置多个anchor box来检测物体。</p>
<p>3.相同的主干网络下，ATSS方法能够大幅增加准确率，十分有效</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/05/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="篮球架上打砖块">
      <meta itemprop="description" content="Apodidae">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Upperlan">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/" class="post-title-link" itemprop="url">docker常用操作</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-03-05 22:18:54 / 修改时间：22:20:25" itemprop="dateCreated datePublished" datetime="2022-03-05T22:18:54+08:00">2022-03-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%B7%A5%E4%BD%9C%E5%B8%B8%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">工作常用</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="docker操作"><a href="#docker操作" class="headerlink" title="docker操作"></a>docker操作</h1><h2 id="docker基本命令"><a href="#docker基本命令" class="headerlink" title="docker基本命令"></a>docker基本命令</h2><ul>
<li>查看容器镜像列表 docker images</li>
<li>查看当前所有正在运行的容器 docker ps  (查看所有容器，包括运行和停止的。t)<h4 id="docker-pull"><a href="#docker-pull" class="headerlink" title="docker pull"></a>docker pull</h4></li>
<li>将网上或者别的服务器上的docker镜像拉到本地。例如： docker pull uber/horovod:0.15.1-tf1.11.0-torch0.4.1-py3.</li>
</ul>
<h4 id="docker-push"><a href="#docker-push" class="headerlink" title="docker push"></a>docker push</h4><ul>
<li><p>上传docker镜像(注意上传镜像统一格式，yq01-aip-m12-tianzhi14.yq01.baidu.com/[Name]:tagls<br>$ docker push yq01-aip-m12-tianzhi14.yq01.baidu.com/lvhaijun/tsm_horovod:0.15.0-tf1.11.0-torch0.4.1-py2.7</p>
</li>
<li><p>建立一个带gpu驱动的容器：</p>
</li>
<li>nvidia-docker run -e PASSWORD=tianzhi05 —network=host —shm-size=32g —name=haijunlv -v /home/ssd1/lvhaijun:/home/lvhaijun -it haijun_tf1.11_torch0.4_py2.7:latest /bin/bash 注意建立容器时请一定将—name 带上，用以区分创建者。无名字的/默认名字的，如果容器在被杀掉概不负责</li>
</ul>
<p>-e：设置环境变量<br>—network：将容器联网<br>—shm-size：/dev/shm/的大小（共享内存）<br>—name：给容器命名<br>-v：将宿主机目录挂载进docker，冒号左边是Host的路径，右边是容器内的路径<br>-it：指示Docker分配连接到容器的伪TTY<br>-p: 指定端口映射，格式为：主机(宿主)端口:容器端口<br>最后是镜像名称:镜像版本</p>
<p>$docker run —runtime=nvidia —shm-size=32g —name=dingzhenkai -p 8017:8017 -v /home/dingzhenkai:/home/dingzhenkai -it registry.baidubce.com/rudder/train_basic/paddle2.1.2-gpu-cuda10.1-cudnn7:basic /bin/bash</p>
<p>注意建立容器时请一定将—name 带上，用以区分创建者。</p>
<ul>
<li><p>退出容器： exit （杀死该容器进程） Control + p+ q (不kill 进程， 只退出当前容器，高频使用)</p>
</li>
<li><p>重新进行一个运行中的容器 docker attach CONTAINER_ID</p>
</li>
<li>从容器进程中创建一个容器进程(某进程死机后可用此方法再创建进程，查看死机进程状况) docker exec -it CONTAINER_ID bash</li>
<li>重新启动一个停止的容器 docker start CONTAINER_ID</li>
<li>docker 容器删除 docker container rm CONTAINER_ID </li>
<li>docker 镜像删除 docker image rm IMAGE_ID</li>
<li>docker rename 重命名</li>
<li>docker exec —privileged -u root -it container-id bash root权限进入容器</li>
<li>docker system df -v 查看容器占用空间</li>
<li>批量静止容器删除 #删除所有未运行的容器（已经运行的删除不了，未运行的就一起被删除了） sudo docker rm $(sudo docker ps -a -q)</li>
<li>docker 服务重启 service docker restart</li>
<li>docker 容器保存成镜像 doker commit <container_id> new_image_name:tag</li>
<li>镜像保存到本地 docker save image_name -o 本地路径/—.tar</li>
<li>镜像从本地加载 docker load -i 本地路径/—.tar</li>
<li>Dead 容器删除:<br>$ docker rm $(docker ps —all -q -f status=dead)</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/05/tmux%E4%BD%BF%E7%94%A8%E7%AE%80%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="篮球架上打砖块">
      <meta itemprop="description" content="Apodidae">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Upperlan">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/tmux%E4%BD%BF%E7%94%A8%E7%AE%80%E8%BF%B0/" class="post-title-link" itemprop="url">tmux使用简述</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-05 22:16:44" itemprop="dateCreated datePublished" datetime="2022-03-05T22:16:44+08:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-03-08 17:11:51" itemprop="dateModified" datetime="2022-03-08T17:11:51+08:00">2022-03-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%B7%A5%E4%BD%9C%E5%B8%B8%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">工作常用</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="tmux"><a href="#tmux" class="headerlink" title="tmux"></a>tmux</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>命令行的典型使用方式是，打开一个终端窗口（terminal window，以下简称”窗口”），在里面输入命令。用户与计算机的这种临时的交互，称为一次”会话”（session） 。</p>
<p>会话的一个重要特点是，窗口与其中启动的进程是连在一起的。打开窗口，会话开始；关闭窗口，会话结束，会话内部的进程也会随之终止，不管有没有运行完。</p>
<p>一个典型的例子就是，SSH 登录远程计算机，打开一个远程窗口执行命令。这时，网络突然断线，再次登录的时候，是找不回上一次执行的命令的。因为上一次 SSH 会话已经终止了，里面的进程也随之消失了。</p>
<p>为了解决这个问题，会话与窗口可以”解绑”：窗口关闭时，会话并不终止，而是继续运行，等到以后需要的时候，再让会话”绑定”其他窗口。</p>
<h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><p>（1）它允许在单个窗口中，同时访问多个会话。这对于同时运行多个命令行程序很有用。</p>
<p>（2） 它可以让新窗口”接入”已经存在的会话。</p>
<p>（3）它允许每个会话有多个连接窗口，因此可以多人实时共享会话。</p>
<p>（4）它还支持窗口任意的垂直和水平拆分。</p>
<h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><p>键入tmux命令，就进入了 Tmux 窗口。<br>按下ctrl+d或者输入exit可以退出</p>
<h4 id="前缀键"><a href="#前缀键" class="headerlink" title="前缀键"></a>前缀键</h4><p>Tmux 窗口有大量的快捷键。所有快捷键都要通过前缀键唤起。默认的前缀键是Ctrl+b，即先按下Ctrl+b，快捷键才会生效。</p>
<p>举例来说，帮助命令的快捷键是Ctrl+b ?。它的用法是，在 Tmux 窗口中，先按下Ctrl+b，再按下?，就会显示帮助信息。</p>
<p>然后，按下 ESC 键或q键，就可以退出帮助。</p>
<h4 id="新建会话"><a href="#新建会话" class="headerlink" title="新建会话"></a>新建会话</h4><p>第一个启动的 Tmux 窗口，编号是0，第二个窗口的编号是1，以此类推。这些窗口对应的会话，就是 0 号会话、1 号会话。</p>
<p>使用编号区分会话，不太直观，更好的方法是为会话起名。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">tmux new -s &lt;session-name&gt;</span> </span><br></pre></td></tr></table></figure></p>
<h4 id="分离会话"><a href="#分离会话" class="headerlink" title="分离会话"></a>分离会话</h4><p>在 Tmux 窗口中，按下Ctrl+b d或者输入tmux detach命令，就会将当前会话与窗口分离。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">tmux detach</span></span><br></pre></td></tr></table></figure>
<p>上面命令执行后，就会退出当前 Tmux 窗口，但是会话和里面的进程仍然在后台运行。</p>
<p><code>tmux ls</code>命令可以查看当前所有的 Tmux 会话。</p>
<h4 id="接入会话"><a href="#接入会话" class="headerlink" title="接入会话"></a>接入会话</h4><p>tmux attach命令用于重新接入某个已存在的会话。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">使用会话编号</span></span><br><span class="line"><span class="meta">$ </span><span class="language-bash">tmux attach -t 0</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">使用会话名称</span></span><br><span class="line"><span class="meta">$ </span><span class="language-bash">tmux attach -t &lt;session-name&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="杀死会话"><a href="#杀死会话" class="headerlink" title="杀死会话"></a>杀死会话</h4><p><code>tmux kill-session</code>命令用于杀死某个会话。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">使用会话编号</span></span><br><span class="line"><span class="meta">$ </span><span class="language-bash">tmux kill-session -t 0</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">使用会话名称</span></span><br><span class="line"><span class="meta">$ </span><span class="language-bash">tmux kill-session -t &lt;session-name&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="切换会话"><a href="#切换会话" class="headerlink" title="切换会话"></a>切换会话</h4><p><code>tmux switch</code>命令用于切换会话。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">使用会话编号</span></span><br><span class="line"><span class="meta">$ </span><span class="language-bash">tmux switch -t 0</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">使用会话名称</span></span><br><span class="line"><span class="meta">$ </span><span class="language-bash">tmux switch -t &lt;session-name&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="重命名会话"><a href="#重命名会话" class="headerlink" title="重命名会话"></a>重命名会话</h4><p><code>tmux rename-session</code>命令用于重命名会话。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">tmux rename-session -t 0 &lt;new-name&gt;</span></span><br><span class="line">上面命令将0号会话重命名。</span><br></pre></td></tr></table></figure>
<h4 id="会话快捷键"><a href="#会话快捷键" class="headerlink" title="会话快捷键"></a>会话快捷键</h4><p>下面是一些会话相关的快捷键。</p>
<p>Ctrl+b d：分离当前会话。<br>Ctrl+b s：列出所有会话。<br>Ctrl+b $：重命名当前会话。</p>
<h3 id="tmux-的窗格常用操作"><a href="#tmux-的窗格常用操作" class="headerlink" title="tmux 的窗格常用操作"></a>tmux 的窗格常用操作</h3><p>我一直认为使用 tmux 中的窗格是一件很酷的事情，很多人喜欢 tmux 也是因为窗格功能的存在。</p>
<p>什么是窗格（pane）呢？</p>
<p>前文也提到过，这里在详细描述一下：当前我们的工作区域，一块工作屏幕我们叫做窗口，窗口是可以被分割的，当前的工作区域被分割的一块块区域就是窗格。</p>
<p>每一个窗格我们可以用来干不同的事情，窗格同窗格之间是相互独立的，可以想象我们使用 vim 来搭配 tmux 的窗格功能是不是很酷呢？</p>
<p><strong>切割窗格</strong></p>
<p>切割窗格的命令是：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux split-window </span><br></pre></td></tr></table></figure>
<p>该命令会把当前工作区域分成上下两个小窗格</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux split-window -h</span><br></pre></td></tr></table></figure>
<p>该命令会把当前工作区域分成左右两个窗格</p>
<p>切割窗格的快捷键 <strong>ctrl + b %</strong> 可以快速的左右切割，<strong>ctrl + b “</strong> 可以快速的上下进行切割。</p>
<p><strong>不同窗格间移动光标</strong></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux select-pane  -U</span><br></pre></td></tr></table></figure>
<p>把当前光标移动到上方的窗格</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux select-pane -D</span><br></pre></td></tr></table></figure>
<p>把当前的光标移动的下方的窗格</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux select-pane -L</span><br></pre></td></tr></table></figure>
<p>把当前的光标移动到左边的窗格</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux select-pane -R</span><br></pre></td></tr></table></figure>
<p>把当前的光标移动到右边的窗格</p>
<p>移动窗格光标的快捷键：</p>
<p><strong>ctrl +b <arrow key></strong>例如 ctrl +b ⬆ 会把光标移动到上方的窗格。</p>
<p><strong>ctrl +b ;</strong>光标切换到上一个窗格</p>
<p><strong>ctrl +b o</strong> 光标切换到下一个窗格</p>
<p><strong>交换窗格的位置</strong></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux swap-pane -U</span><br></pre></td></tr></table></figure>
<p>当前窗格向上移动</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux swap-pane -D</span><br></pre></td></tr></table></figure>
<p>当前窗格向下移动</p>
<p><strong>关闭当前的窗格</strong></p>
<p>关闭窗格通常使用快捷键 <strong>ctrl + b x</strong></p>
<p><strong>放大窗格</strong></p>
<p>快捷键 <strong>ctrl + b z</strong> ,将会放大当前操作的窗格，继续触发该快捷键将会还原当前的窗格。</p>
<p><strong>窗格显示时间</strong></p>
<p>快捷键 <strong>ctrl +b t</strong> 将会把在当前的窗格当中显示时钟，非常酷炫的一个功能，点击 enter (回车键将会复原)。</p>
<p><strong>窗格总结</strong></p>
<p>关于窗格的操作我们经常操作的就是分割，移动光标，放大窗格，关闭窗格，可以熟练以上提到的操作，关于移动光标的快捷键操作，下文在 .tmux.conf 中也会处理成快捷键进行操作。</p>
<h4 id="进入tmux翻屏模式"><a href="#进入tmux翻屏模式" class="headerlink" title="进入tmux翻屏模式"></a>进入tmux翻屏模式</h4><p>Crtl+b，松开 按 [ 进入翻屏模式，PgUp和PgDn实现上下翻页。退出按q</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/10/%E8%AE%AD%E7%BB%83%E8%AE%A1%E5%88%92-%E8%BA%AB%E4%BD%93%E7%B4%A0%E8%B4%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="篮球架上打砖块">
      <meta itemprop="description" content="Apodidae">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Upperlan">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/10/%E8%AE%AD%E7%BB%83%E8%AE%A1%E5%88%92-%E8%BA%AB%E4%BD%93%E7%B4%A0%E8%B4%A8/" class="post-title-link" itemprop="url">训练计划-身体素质</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-10 14:35:17" itemprop="dateCreated datePublished" datetime="2022-01-10T14:35:17+08:00">2022-01-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-03-06 13:45:14" itemprop="dateModified" datetime="2022-03-06T13:45:14+08:00">2022-03-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%94%B7%E5%A3%AB%E4%BC%91%E9%97%B2/" itemprop="url" rel="index"><span itemprop="name">男士休闲</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="腿部力量-弹跳爆发Day（周一周四）"><a href="#腿部力量-弹跳爆发Day（周一周四）" class="headerlink" title="腿部力量+弹跳爆发Day（周一周四）"></a>腿部力量+弹跳爆发Day（周一周四）</h3><p>-深蹲4x8（四组每组八下）</p>
<p>每组做完接超级组 深蹲跳 8下（可选择负重）全力跳！</p>
<p>*重量选择自己做八下比较吃力的重量</p>
<p>-负重健步行走 4x16（16步）</p>
<p>每组做完接超级组 健步跳8-10下 全力跳！挑战自己！</p>
<p>-硬拉 4x6-8</p>
<p>每组做完接低高度跳台阶 12-20次 尽可能连续跳跃 不间断 每次一落地就立刻再次跳起来</p>
<p>-推车冲刺 大约30米 或篮球场距离</p>
<p>紧接着徒手冲刺跑回来</p>
<p>做3-5组 每个来回算一组</p>
<p>（假设没有推车的话 就全部徒手冲刺）</p>
<p>全力百分之百速度冲！挑战自己！</p>
<p>-引体抬腿 3x8</p>
<p>（做不到的话 就竭尽所能 做不了的话也竭尽所能努力尝试！挑战自己！）</p>
<p>-平板支撑 两分钟 就一组 </p>
<p>（做不到两分钟的话 就做到自己的极限！竭尽所能！能做到三分钟或更多的话欢迎去挑战！挑战自己！！）</p>
<h3 id="胸背Day-（周二周五）"><a href="#胸背Day-（周二周五）" class="headerlink" title="胸背Day （周二周五）"></a>胸背Day （周二周五）</h3><p>-卧推 3x8 （你做八组比较吃力的重量 但又不是力竭）</p>
<p>-引体向上 3x8 （做不到那么多的话 就做到自己力竭 竭尽全力！做不了引体向上的话 就每次都努力尝试做 或者用辅助带 挑战自己！）</p>
<p>-斜侧推 3x8-12 三组 每组8到12 </p>
<p>-Row 往下拉的器械 3x8 （最后一下 拉下来后多停两秒）</p>
<p>-Cable Chest Press 3x8-12 </p>
<p>每组结束后紧接着做俯卧撑 8-10下</p>
<p>-Row 平拉的器械 3x8 （最后一下 拉下来后停两秒）</p>
<p>-负重引体抬腿 3x8-12（太难的话 可以选择不负重）</p>
<p>-负重仰卧起坐 3x8-12（太难的话 可以选择不负重）</p>
<h3 id="麒麟臂Day（周三周六）"><a href="#麒麟臂Day（周三周六）" class="headerlink" title="麒麟臂Day（周三周六）"></a>麒麟臂Day（周三周六）</h3><p>-哑铃Curl 3x8-10 掌控好节奏 别太快！</p>
<p>-三头Press Down 3x10-12 </p>
<p>-肩部推杠铃 3x8</p>
<p>-杠铃Curl 3x8</p>
<p>-三头Extension 3x12 左右各12</p>
<p>-飞鸟肩 3x8-12</p>
<p>-抬腿卷腹50 紧接 仰卧抬腿 30 紧接 仰卧剪刀腿 50 一组</p>
<p>-滚轮 3x10</p>
<p>周日休息</p>
<p>*休息非常重要！这才是你的身体真正开始进化的时刻！</p>
<p>多！ 拉！ 抻！</p>
<p>*平时多尝试扣篮或最大化摸高，任何时间任何地点，尤其是各种训练后，多跳多跳多全力跳！从摸篮网、篮板、篮筐开始 一步一步到扣篮，根据自己的能力挑战自己，往高了够！多跳！！努力让自己变得兴奋，激活自己的中枢神经区！</p>
<p>*多 拉 抻！恢复与健康是关键！尤其是训练后！这非常重要！！</p>
<p>*饮食：如果你要减脂肪塑形的话 不要吃面包、饮料、甜品、冰淇淋、油炸、猪肉 多吃蔬菜、绿菜、鸡肉、适当的米饭、豆、适当的牛羊肉、水果、大量的白水；如果你是要增肌的话 而且还是作为篮球运动员 送给你两个字：多！吃！别管吃什么 只要不是太过多的垃圾食品 或过多的甜的 多！吃！！</p>
<p>PAIN IS MY FUEL.</p>
<p><a target="_blank" rel="noopener" href="https://v.youku.com/v_show/id_XMzgwMzI1ODk0OA==.html?spm=a2h3j.8428770.3416059.1">https://v.youku.com/v_show/id_XMzgwMzI1ODk0OA==.html?spm=a2h3j.8428770.3416059.1</a></p>
<p>密码：HangtimeStrong</p>
<p>注意大小写</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/10/%E8%AE%AD%E7%BB%83%E8%AE%A1%E5%88%92-%E7%AF%AE%E7%90%83%E6%8A%80%E6%9C%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="篮球架上打砖块">
      <meta itemprop="description" content="Apodidae">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Upperlan">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/10/%E8%AE%AD%E7%BB%83%E8%AE%A1%E5%88%92-%E7%AF%AE%E7%90%83%E6%8A%80%E6%9C%AF/" class="post-title-link" itemprop="url">训练计划-篮球技术</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-10 14:35:17" itemprop="dateCreated datePublished" datetime="2022-01-10T14:35:17+08:00">2022-01-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-03-06 13:44:39" itemprop="dateModified" datetime="2022-03-06T13:44:39+08:00">2022-03-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%94%B7%E5%A3%AB%E4%BC%91%E9%97%B2/" itemprop="url" rel="index"><span itemprop="name">男士休闲</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一-Elite"><a href="#一-Elite" class="headerlink" title="一. Elite"></a>一. Elite</h2><p>原地普通运球 左右手各50<br>原地V字运球 左右手各50<br>原地普通运球+体前变向 30<br>原地普通运球+胯下 30<br>小幅度变向 50<br>背后运球 50<br>原地胯下背后Combo 左右侧各 15<br>原地胯下体前Combo 左右侧各 20<br>原地连续剪刀胯下 50<br>原地体前+胯下+背后 50<br>胯下体前胯下背后 左右各50</p>
<ul>
<li><strong>运球一定要使劲儿！用全力！</strong></li>
<li><strong>不要怕丢！大家都会有失误很正常，追求完美，努力克服自己、克服困难！</strong></li>
<li><strong>做不了的动作 就努力尝试！老去尝试 去挑战自然而然就会了！</strong></li>
<li><strong>运球注意抬头！Eyes Up！</strong></li>
<li><strong>压低重心，屈膝</strong>！</li>
</ul>
<p>行进中运球（底线到三分线的距离来回）<br>Sham+双体前<br>胯下行进<br>背后行进<br>In n Out胯下体前<br>胯下+双背后<br>In N Out Crossover<br>双体前+胯下+双背后</p>
<ul>
<li>新手 努力尝试！</li>
<li>不熟的话可以先把动作慢慢做 熟悉后加快速率！</li>
<li>高手 尽全力 干！！</li>
</ul>
<p><strong>以下所有数字全部为要求 -进-球-数- ！</strong></p>
<p>篮筐底下投篮 10 （争取空心）<br>弧顶顺步突破抛投 左右各 8<br>弧顶试探步突破转身上篮 左右各5<br>右-突破胯下拉回跳投 5<br>右-突破胯下拉回变向拜佛 5 正向拜佛 5<br>左-突破胯下拉回跳投 5<br>左-突破胯下拉回变向拜佛 5 正向拜佛 5<br>*拜佛后的处理 自己选择！放大自己的想象力！各种各样的上篮！<br>罚球 3<br>认真对待每一个球！</p>
<p>面对挡拆的处理<br>-右侧-<br>过挡拆后直接投篮 5 （当防守者从挡拆后面绕）<br>过挡拆后犹豫步突破 5 （当补防大个没有及时防你）ps：突破后 各种上篮或抛投 自己选择<br>过挡拆前变向跳投 5 （当防守者自作聪明提前绕挡拆）<br>过挡拆后Split突破 5 （当补防者过于凶猛地扑你）ps：突破后的选择 自己发挥想象力！<br>过挡拆后后撤 胯下体前 8 （当对手选择直接换防）接下来跳投或突破 自己选择<br>-左侧-<br>过挡拆后直接投篮 5 （当防守者从挡拆后面绕）<br>过挡拆后犹豫步突破 5 （当补防大个没有及时防你）ps：突破后 各种上篮或抛投 自己选择<br>过挡拆前变向跳投 5 （当防守者自作聪明提前绕挡拆）<br>过挡拆后Split突破 5 （当补防者过于凶猛地扑你）ps：突破后的选择 自己发挥想象力！<br>过挡拆后后撤 胯下体前 8 （当对手选择直接换防）接下来跳投或突破 自己选择<br>罚球 3<br>在疲累的时候 罚球 更考验真本事！</p>
<p>推进<br>In n out Crossover 跳投 左右各 5<br>哈登步突破抛投 左右各 5<br>哈登步接胯下突破上篮 左右各 5<br>In n out 接急停跳投 左右各 8<br>*投篮射程按照自己的能力与需求定哦<br>罚球 5</p>
<h2 id="二-Good-Times"><a href="#二-Good-Times" class="headerlink" title="二. Good Times"></a>二. Good Times</h2><p>半场距离往返运球<br>-进攻+后退+胯下运球进攻<br>-行进间背后运球<br>-胯下突破背后拉回<br>-双变向Hesi胯下<br>-In n Out 背后运胯下拉回</p>
<p>胯下突破上篮 左右各 3<br>胯下突破抛投 左右各 3<br>顺步突破反篮 左右各 3 （可用诶诶步）<br>胯下hesi crossover 左右翼各 3 （变向要学会用眼神！）<br>双变向突破rondo假上篮 左右各 2<br>罚球 5</p>
<p>三威胁一次运球急停跳投 左右各 5<br>转身后12步急停跳投 左右各 5<br><em>自己定射程！根据自己的需求与能力</em><br>胯下Hesi急停跳投 左右各 5<br>胯下Hesi一次运球急停跳投 左右各 3<br>In n Out胯下12步急停跳投 左右各 3<br>胯下Hesi Crossover急停跳投 左右各 5<br>胯下紧接干拔 左右各 5<br>胯下突破背后拉回跳投 左右各 5<br>变向拜佛突破抛投 左右翼各 5<br>哈登步Crossover顿拜突破 左右各 3<br>超级后撤步 左右各 3<br>利拉德后撤步 左右各 5<br>IT突破接后撤步跳投 左右翼各 5<br>In n Out 一次运球急停跳投 左右翼各 5<br>In n Out 接拜佛突破 8<br>胯下加胯下后跳步跳投  5<br>罚球 10 </p>
<h2 id="三-Project-A"><a href="#三-Project-A" class="headerlink" title="三. Project A"></a>三. Project A</h2><p>双球 交替运50<br>双球 齐v字50<br>双球交替v字50<br>双球 齐侧v字50<br>双球交替侧v字50<br>单球 低运绕八字 100<br>单球 低运反绕八字 100<br>行进间 胯下变向 半场来回1次<br>行进间 背后变向 半场来回1次<br>行进间 体前变向 半场来回1次<br>行进间 in n out crossover 半场来回 1次<br>行进间 胯下+体前 半场来回1次<br>行进间 胯下+背后 半场来回1次<br>行进间 背后+背后 半场来回1次<br>5罚球<br>篮下miken训练<br>双脚正篮 10 反篮 10<br>单脚正篮 10 反篮 10<br>篮筐底下投篮（要求空心）10<br>弧顶持球<br>1次运球 急停跳投 左右各5<br>1次运球突破 背后拉回 跳投 左右各5<br>1次 运球突破 反胯下拉回 跳投 左右各5<br>胯下+体前crossover 跳投 左右各10<br>胯下+体前crossover 抛投 左右各8<br>哈登步突破上篮 左右各 5<br>哈登步急停跳投 左右各8</p>
<p>左右翼<br>半转身走底线突破上篮 左右各5<br>hesi 突破底线 背后拉回跳投 左右各5<br>双体前crossover 突破底线反篮 左右各5</p>
<p>In n out Crossover 跳投 左右各5<br>In n out Crossover 反胯下拉回 左右各5</p>
<p>左右翼Again<br>双体前crossover 抛投 左右各5<br>双体前 crossover 后撤步 左右各5<br>背后+体前crossover 跳投 左右各5<br>背后+体前crossover 突破欧洲步 左右各5<br>10罚球</p>
<ul>
<li>以上分别是ELITE、Good Times、PROJECT A 三个板块，不同的训练结构、动作、方法、技能，可以选择每天换一个板块练，也可以选择每周 或 每月换一个板块练，根据自己的水平与需求！</li>
<li>每一个板块的内容，是每一次训练（每天）需要完成的量，这是最理想的状态。</li>
<li>如若说一天完成不了 某板块里面规定的内容，可以根据自己的情况 分两天（最多三天）完成，挑战自己！克服困难！</li>
<li>训练过程中你会发现很多自己的不足，克服他们！保持稳定的训练！</li>
<li>假设说里面某个动作让你非常喜欢，你也完全可以加量练习它！</li>
<li>无论是哪个板块，争取每天都抽一个做一遍！或每周做四次以上！天道勤酬！</li>
</ul>
<p>Never Give Up！<br>When you put your heart and mind in to Anything, God will you help you achieve it.<br>与君共勉！</p>
<p><a target="_blank" rel="noopener" href="https://v.youku.com/v_show/id_XMzgwNzM1NjI5Mg==.html?spm=a2h3j.8428770.3416059.1">https://v.youku.com/v_show/id_XMzgwNzM1NjI5Mg==.html?spm=a2h3j.8428770.3416059.1</a></p>
<p>密码：thankyouGod<br>注意大小写<br>00:00 - 20:15 Elite<br>20:16 - 32:47 Good Times<br>32:48 - 50:40 Project A</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/02/YOLO%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="篮球架上打砖块">
      <meta itemprop="description" content="Apodidae">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Upperlan">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/02/YOLO%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">YOLO系列模型</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-02 20:40:25" itemprop="dateCreated datePublished" datetime="2021-11-02T20:40:25+08:00">2021-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-03-14 17:23:41" itemprop="dateModified" datetime="2022-03-14T17:23:41+08:00">2022-03-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" itemprop="url" rel="index"><span itemprop="name">目标检测</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<h3 id="几个宏观上的理解"><a href="#几个宏观上的理解" class="headerlink" title="几个宏观上的理解"></a>几个宏观上的理解</h3><ul>
<li>从直觉上理解，两阶段模型做了粗挑+精挑的操作。 那在目标物体的召回率上会有所下降，在精确率上会有所提升。<br>单阶段模型只有一个阶段，在物体的召回率上会比较高，但在精确率上会有所下降。</li>
<li>业务上会根据需求来挑模型，如果要求高精度，就用frcnn，精度和性能平衡，就用yolo。</li>
</ul>
<h2 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a>YOLOv1</h2><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h3><p>YOLO是一个单独的end-to-end网络，其本质是吧目标检测问题当做一个回归问题去解决。</p>
<p>整体模型图如下：</p>
<p><img src="image-20220305211422793.png" alt="image-20220305211422793"></p>
<h4 id="backbone"><a href="#backbone" class="headerlink" title="backbone"></a>backbone</h4><p>基于GoogLeNet model，24个卷积层和两个全连接层，前20层在image1000上预训练，然后连上剩下的卷积层和全连接层再训练。</p>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li><p>是一个unified模型即整体性-&gt;把识别bbox，识别标签两个功能集成到一个网络中</p>
</li>
<li><p>训练时能够看到整个图片-&gt;即端到端，输出层是从整个特征图，其中每个位置上的通道就反应了这个位置的图像特征，因此yolo会比rcnn更少犯背景识别错误<br>注解：和两阶段检测模型不同，两阶段模型是先筛选ROI区域，此时卷积还不够深，还比较浅所以容易误识别背景为目标提取出来了；单阶段模型输出的特征图反应的是感受野区域上与目标物体相关的信息，输出时层数更深，所以对背景和物体的分辨能力更强，不容易犯背景误识别的错误。</p>
</li>
<li><p>学习的是总体上的特征-&gt;对于输入的鲁棒性强</p>
</li>
<li><p>虽然精度离其他模型还有一些差距，但是贵在快</p>
</li>
</ul>
<p>最后训练完备的模型推理时的输出：</p>
<ul>
<li><p>输入：原始图像resize后的图像</p>
</li>
<li><p>输出：物体位置和类别</p>
</li>
</ul>
<p><strong>注意：</strong> RCNN系列模型是将检测模块分为两部分，物体类别是一个分类问题；物体位置（即求bbox）是个回归问题</p>
<h4 id="2-1输入输出定义"><a href="#2-1输入输出定义" class="headerlink" title="2.1输入输出定义"></a>2.1输入输出定义</h4><p>输入：一个batch的image，shape是[batch_size,3,448,448]</p>
<p>输出：shape是[batch_size，Bboxs * 5 + Classes，size， size]</p>
<p>把图像分成S * S个grid cell，当有物体的中心落到一个cell中时（怎么判断？-&gt;confidence值），这个cell就要负责检测这个物体。每个cell要预测B个bbox，每个bbox都有一个相应的confidence</p>
<p>预测时，每个Bounding box包含五个数据值：</p>
<ul>
<li><p>x,y为当前格子预测到的物体的bbox中心位置坐标。</p>
</li>
<li><p>w和y为这个格子的高度和宽度</p>
</li>
<li><p>confidence反应当前bbox包含物体的置信概率</p>
</li>
</ul>
<p>confidence = P(object) * IOU(交并比)</p>
<p>若包含物体，P（）=1，否则为0。所以预测时输出的p值就是预测了这个bbox和object的IoU</p>
<p>每个grid cell预测C个种类，每个种类有个概率P值，每个grid cell只输出一组预测值</p>
<p>条件概率P（class i | Object）这个概率是基于grid cell包含object的概率为前提的，所以P（class i）=P（class i | Object）* P（object）</p>
<p>最后得到class-specific confidence = P（class i） * IOU，这个值说明了box中出现相应类的概率，也表征了预测的bbox对目标的拟合程度</p>
<p>所以输出的维数是S <em> S </em> （B * 5 + C）</p>
<p>在test时，在object所在的grid cell，yolo每次都会预测许多bbox，每个预测bbox都包含了一个预测的IOU大小，选取有更大IOU的bbox就会被选出来成为检测出来的框，预测的类别是每个grid cell中最大的那个Ci值</p>
<h4 id="2-2-损失函数"><a href="#2-2-损失函数" class="headerlink" title="2.2 损失函数"></a>2.2 损失函数</h4><p>损失分为两部分，bbox和种类预测。如果设为相同比重来构成总得损失函数不太理想-&gt;因为有的grid cell并不包含有效内容-&gt;影响了（overpower）包含目标物体的cell的梯度信息，导致confidence很低，所以将bbox损失的权重调整为5，confidence损失的权重调整为0.5。（应该是调出来的值）这样的直接结果就是大bbox中小的偏差相对于小bbox中小的偏差对损失影响较小。-&gt;为了实现这个目的，我们预测的值是width和height值的平方根</p>
<p><img src="image-20220314150921131-7241762.png" alt="image-20220314150921131"></p>
<h3 id="3训练过程"><a href="#3训练过程" class="headerlink" title="3训练过程"></a>3训练过程</h3><p>YOLO模型训练分为两步：</p>
<p>1）预训练。使用ImageNet 1000类数据训练YOLO网络的前20个卷积层+1个average池化层+1个全连接层。训练图像分辨率resize到224x224。训练一周，单个corp（应该是边框预测）88% top5准确率</p>
<p>2）用步骤1）得到的前20个卷积层网络参数来初始化YOLO模型前20个卷积层的网络参数，加四个卷积层和两个全连接层，随机初始化这几层的权重，然后用VOC 20类标注数据进行YOLO模型训练。为提高图像精度，在训练检测模型时，将输入图像分辨率resize到448x448。</p>
<p>3）最后一层的输出是归一化后的width和height，x，y</p>
<p>NOTE：只有当object在相应cell中时才会有分类损失，也只有最高IOU的predictor找到后才会有bbox相应的损失惩罚</p>
<p>训练时一开始学习率要小一点，慢慢升高，最后再降下来</p>
<p>使用了dropout来尽量避免过拟合</p>
<p>非极大抑制（non-maximal suppression）解决检测结果中，有多个区域重叠问题，增大了ap</p>
<h4 id="Limitation"><a href="#Limitation" class="headerlink" title="Limitation"></a>Limitation</h4><p>由于空间约束的原因，很难识别小的成群的目标</p>
<p>采用的特征相对比较粗糙</p>
<p>对于小目标的的识别可能定位不太准确</p>
<h4 id="模型对比"><a href="#模型对比" class="headerlink" title="模型对比"></a>模型对比</h4><p>RCNN使用一个region proposals来候选出待识别的对象，再用selective Search来生成大概2k个bbox，通过一个cnn来提取特征，再用svm对特征进行分类评分，再用线性模型调整bbox，非极大抑制来减少多重框，每个阶段都要单独调参，但是精度相对比较高。yolo一般就生成98个bbox，速度会快很多，而且是一个unified模型，运行也很快。</p>
<p>相对来说能够减少将背景误识别成object的概率</p>
<h2 id="YoloV2（Yolo9000）"><a href="#YoloV2（Yolo9000）" class="headerlink" title="YoloV2（Yolo9000）"></a>YoloV2（Yolo9000）</h2><p>存在的问题：相比于f-rcnn，预测框不准，不全，</p>
<h3 id="改良步骤："><a href="#改良步骤：" class="headerlink" title="改良步骤："></a>改良步骤：</h3><p>1.加了BN层：mAp加了2%，丢掉了dropout</p>
<p>2.分类器：用了在ImageNet上预训练的模型。原始的分类器是在224 <em> 224的图像上预训练，然后检测448 </em> 448的图像，现在改成预训练后先在448 * 448的图上fine-tune。这个操作加了%4的mAP</p>
<p>3.卷积的过程中加入了anchor box：yolov1单纯在卷积层之后使用全链接层才获取特征。现在移除全连接层，变成了对每个anchor box的种类和目标位置进行预测，目标检测依然是预测P值和框的属性值，类检测依然是预测此处物体的类别。简而言之，就是v1是对每个grid预测一次种类，现在是对每个anchor预测一次种类。对原始数据集里的框框大小来一次k均值分类来选出频率最高的几种框框类型。</p>
<p><strong>=================================</strong></p>
<p><strong>名词解释：</strong></p>
<p>anchor box：原本一个格子只能检测出一个对象，anchor box能让一个格子检测出多个对象。预先定义几个不懂形状的anchor box。假设普通yolo中一个bbox的预测输出y包含object的中心位置（横纵坐标x，y），bbox的宽和高（w，h），是否存在object的置信概率p，各个类别的预测值（n种类别就有n个输出）。那么普通yolo对于一个bbox的预测输出就是5+n维数据。</p>
<p>而anchor是对多个bbox进行预测，假设预测m个bbox，则输出y为m * (5+n)维数据。每5+n维数据都与一个bbox相关联。</p>
<p>所以现在就是根据object中心点分配到一个格子中，然后看gt和每个anchor bbox的IOU，选取IOU最大的anchor bbox来预测。</p>
<p><strong>=================================</strong></p>
<p>4.anchor box遇到的问题</p>
<p>5.直接位置预测：直接预测x，y坐标时可能会出现预测的坐标在grid cell以外的情况，不稳定，所以改成预测下图中的t值，</p>
<p><img src="anchor_offset.png" alt="image-20220305205145344"></p>
<p>预测框就是在anchor box的基础上进行微调</p>
<p>6.更精细的特征：用了一个passingthrough layer重排特征，有利于小目标识别</p>
<p>7.多重大小图像训练：所以虽然每个batch之内图像的尺寸必须是相同的，但是不同的batch之间图像的尺寸是不受限于框架的。YOLOv2便是基于这点实现了其训练过程中的多尺度。</p>
<p>8.语义分层： 在预测物体的类别时，我们遍历整个WordTree，在每个分割中采用最高的置信度路径，直到分类概率小于某个阈值（源码给的是0.6）时，然后预测结果，可以检测coco中没有的标签，类似于一种半监督学习</p>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>Darknet-19</p>
<p>包括了19个卷积层和5个maxpooling层</p>
<p><img src="darknet.png" alt="image-20220305205225977"></p>
<p><img src="image-20220314155601017-7244563.png" alt="image-20220314155601017"></p>
<p>route层就是将两个或多个卷积层的结果做拼接</p>
<p>这里再放一张和yolov1标注的对比</p>
<p><img src="image-20220314155700031-7244621.png" alt="image-20220314155700031"></p>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><h4 id="训练分类"><a href="#训练分类" class="headerlink" title="训练分类"></a>训练分类</h4><p>预训练，ImageNet 1000训练160个epochs。</p>
<p>fine tune，由原来的224变成448</p>
<h4 id="训练检测"><a href="#训练检测" class="headerlink" title="训练检测"></a>训练检测</h4><p>去掉最后一个卷积层，加了三个3 <em> 3 卷积层，每个卷积层后有一个1 </em> 1 的卷积层输出我们想要的结果</p>
<h3 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h3><p><img src="image-20220314155726577-7244651.png" alt="image-20220314155726577"></p>
<h2 id="Yolov3"><a href="#Yolov3" class="headerlink" title="Yolov3"></a>Yolov3</h2><p>先放一张网络结构图</p>
<p><img src="image-20220314155800077-7244681.png" alt="image-20220314155800077"></p>
<p>1.多尺度预测，步长不同，预测物体尺寸不同</p>
<p>2.既发挥了深层网络的语义特化抽象特征（大目标），又利用了浅层网络的细粒度的底层特征（小目标）。</p>
<h3 id="bbox预测"><a href="#bbox预测" class="headerlink" title="bbox预测"></a>bbox预测</h3><p>预测的参数还是和V2相似，但是目标检测分数（类似于IOU，表征这个bbox是否是目标bbox）改成了用逻辑回归来输出，如果有一个bbox比其他所有bbox覆盖的gt面积更多的话，他的值就是1</p>
<h3 id="网络设计"><a href="#网络设计" class="headerlink" title="网络设计"></a>网络设计</h3><h4 id="backbone-1"><a href="#backbone-1" class="headerlink" title="backbone"></a>backbone</h4><p>Darknet-53</p>
<h4 id="多尺度预测FPN-feature-pyramid-network"><a href="#多尺度预测FPN-feature-pyramid-network" class="headerlink" title="多尺度预测FPN(feature pyramid network)"></a>多尺度预测FPN(feature pyramid network)</h4><p>当前层的feature map会对更高层的feature map进行上采样，并加以利用。这是一个有跨越性的设计。因为有了这样一个结构，当前的feature map就可以获得更高层，更抽象的信息，这样的话低阶特征与高阶特征就有机融合起来了，提升检测精度。</p>
<p><img src="image-20220314160013157-7244814.png" alt="image-20220314160013157"></p>
<p>步幅越大，特征图尺寸越小，每个像素点感受野很大，有丰富的高层语义信息，容易检测到大目标，但是对小目标识别效果不太好，小目标需要在尺寸较大的特征图上面建立预测输出。</p>
<p>在目标检测中，解决这一问题的方式是，将高层级的特征图尺寸放大之后跟低层级的特征图进行融合，得到的新特征图既能包含丰富的语义信息，又具有较多的像素点，能够描述更加精细的结构。</p>
<p>深层一般具有更丰富的语义信息，但精度低，稍微浅一点的层含的语义信息会少一些，但是精度高。所以把高层的特征图通过卷积，upsample后与稍微低一点的层拼接，就有比较丰富的语义，p1和p2在空间和语义含义上都更具有优越性</p>
<h3 id="目标检测模型模块化"><a href="#目标检测模型模块化" class="headerlink" title="目标检测模型模块化"></a>目标检测模型模块化</h3><p><img src="image-20220314160406858-7245052.png" alt="image-20220314160406858"></p>
<p>yolov3开始基本上可以将目标检测模型分成backbone，Neck，head这几个大的模块，backbone是用来学习目标对象特征的基础，neck主要是关于模型多尺度识别的结构，head在其他模型上有开启下游任务的结构，也有anchor-free系列模型会在head上做一些设计</p>
<h3 id="模型输出"><a href="#模型输出" class="headerlink" title="模型输出"></a>模型输出</h3><p><img src="image-20220314160454314-7245095.png" alt="image-20220314160454314"></p>
<p>输出的是特征图，不是原来yolov1输出的序列，在每个特征图上，根据步长大小在每个相应感受野的point上生成三个预测框，每个预测框包括bbox位置信息，objectness物体是否存在置信度，以及物体类别的置信度</p>
<h4 id="类别置信度"><a href="#类别置信度" class="headerlink" title="类别置信度"></a>类别置信度</h4><p>类别置信度表示检测到的对象属于某个类别的概率（如狗、猫、香蕉、汽车等）。YOLO v3 使用 sigmoid 函数。因为对类别分数执行 softmax 操作的前提是类别是互斥的。简言之，如果对象属于一个类别，那么必须确保其不属于另一个类别。这在我们设置检测器的 COCO 数据集上是正确的。但是，当出现类别「女性」（Women）和「人」（Person）时，该假设不可行。这就是作者选择不使用 Softmax 激活函数的原因。</p>
<h4 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h4><p><img src="image-20220314160946001-7245388.png" alt="image-20220314160946001"></p>
<h3 id="正负样本分配"><a href="#正负样本分配" class="headerlink" title="正负样本分配"></a>正负样本分配</h3><p><img src="image-20220314161210057-7245531.png" alt="image-20220314161210057"></p>
<p>在训练过程中，论文上表达的是，模型会先根据在每个grid cell上预先设计好的anchor与gt框做IOU计算，然后选择某个步长上的某个特定的grid cell上的特定的bbox作为这个gt框的正样本（因此正样本很少），其余的检测框中，如果有检测框与gt框的IOU比较大（如大于0.7），就直接标记为负（这样其实漏掉了很多检测很好的样本，过于依赖先验知识了）</p>
<h2 id="Yolov3的一些代码逻辑"><a href="#Yolov3的一些代码逻辑" class="headerlink" title="Yolov3的一些代码逻辑"></a>Yolov3的一些代码逻辑</h2><p>yolov3对图像实现目标检测目标，其模型本质的功能是基于输入的图像生成对该图像上检测目标的标签的值。</p>
<p>假设一张图像被分成13 * 13 = 169个grid cells，每个cell生成3个anchor box</p>
<p>标签的值包括：bbox的位置相对值，objectness值，类别预测值。</p>
<p>特点：通过算数转换，模型计算损失时所有的标签值序列里每一个值都是由sigmoid输出0-1之间的相对值。</p>
<p>输入的单个样本：一张图像</p>
<p>对应标签：由图像生成anchor box 数据，每个anchor box数据包括上述标签的值，即每个图片的标签包含是13 <em> 13 </em> 3 <em> （4+1+clsss）个0-1之间的值，因此我们的模型输出也是13 </em> 13 <em> 3 </em> （4+1+clsss）个0-1之间的值，后续我们通过iou计算，objectness阈值，nms等处理我们模型输出的值，得到较为直观的结果。</p>
<p>所以yolov3整体的流程为：</p>
<p>图像数据resize-&gt;图像增广-&gt;图像上anchor box标签生成-&gt;将输入样本和其标签对应起来-&gt;输入模型-&gt;输出数据后选择正负样本与检测框计算loss，处理，根据模型输出计算出预测框的xywh，种类等</p>
<h3 id="1-预处理"><a href="#1-预处理" class="headerlink" title="1 预处理"></a>1 预处理</h3><h4 id="1-1-图像读取"><a href="#1-1-图像读取" class="headerlink" title="1.1 图像读取"></a>1.1 图像读取</h4><p>用cv包读取后resize成416（为了让图像中心正好有一个grid cell），通道顺序注意一下，加一个batch_size通道,顺便归一化，把图像数值化成tensor变成相应框架的变量。</p>
<h4 id="1-2-anchor-box的生成与标注标签"><a href="#1-2-anchor-box的生成与标注标签" class="headerlink" title="1.2 anchor box的生成与标注标签"></a>1.2 anchor box的生成与标注标签</h4><h5 id="anchor-box标签生成"><a href="#anchor-box标签生成" class="headerlink" title="anchor box标签生成"></a>anchor box标签生成</h5><p>其实就是需要给每个grid cell生成指定的height和width的anchor box。bbox所需的参数即xywh：中心坐标和长宽数据。而在实际生成预测框时这四个数据都要进行一定程度的改变来指向特定物体</p>
<h5 id="划分单位size的grid-cell"><a href="#划分单位size的grid-cell" class="headerlink" title="划分单位size的grid cell"></a>划分单位size的grid cell</h5><p>列数：</p>
<script type="math/tex; mode=display">m = \frac{height}{size}</script><p>行数：</p>
<script type="math/tex; mode=display">n = \frac{width}{size}</script><h5 id="针对objectness单独的理解"><a href="#针对objectness单独的理解" class="headerlink" title="针对objectness单独的理解"></a>针对objectness单独的理解</h5><p>针对每一个图，所有的anchor box放在一起相当于一个小的数据集，其中有一些样本的objectness标签为1，有一些样本的objectness标签为0。让模型学习类似f（box参数+图像）=objectness这样的函数关系，从而让模型能够预测anchor box的objectness值</p>
<p>这里的标签是计算出来的，计算方法是计算每个anchor box和每个gt物体框的iou，同一个物体中心只存在于一个grid cell，iou超过阈值且最大的那个anchor box标签为1，超过阈值但不是最大的box标签为-1（不参加训练objectness模型），剩下的标签为0。所以同一个物体只能选一个anchor box打上objectness = 1的标签。将图像数据和box的参数作为输入，objectness值作为输出进行训练，让模型学习类似f（box参数+图像）=objectness这样的函数关系，所以训练好的抽象模型就可以根据输入的图像数据和box参数判断objectness的值。</p>
<p>当一个框的objectness为0或-1时，不用再去标注下面的box标签参数了。</p>
<h5 id="由anchor-box到预测框：中心坐标"><a href="#由anchor-box到预测框：中心坐标" class="headerlink" title="由anchor box到预测框：中心坐标"></a>由anchor box到预测框：中心坐标</h5><p>因为每个objectness = 1的anchor box都是批量生成好的，不可能和物体恰好重合，所以检测时肯定会微调，由anchor box到检测的bbox。此时为了方便，技术单位统一变成grid cell的size，所以x行，y列的位置坐标其实为 <script type="math/tex">(x + \sigma(t_{x})) \times size</script> 和 <script type="math/tex">(y + \sigma(t_{y})) \times size</script> 其中 $\sigma()$ 可以是一个sigmoid函数，输出是0到1之间的数。因此我们可以通过改变 $\sigma()$ 来改变anchor的中心坐标，且保证中心坐标一定在规定的grid cell里面。</p>
<h5 id="由anchor-box的-p-w-p-h-到预测框：w-h"><a href="#由anchor-box的-p-w-p-h-到预测框：w-h" class="headerlink" title="由anchor box的 $p_{w}$,$p_{h}$ 到预测框：w,h"></a>由anchor box的 $p_{w}$,$p_{h}$ 到预测框：w,h</h5><script type="math/tex; mode=display">w = p_{w}e^{t_{w}}</script><script type="math/tex; mode=display">h = p_{h}e^{t_{h}}</script><p>其中$t$是参数</p>
<h5 id="而anchor-box到预测框之间的这些参数就是我们模型希望能够预测的值"><a href="#而anchor-box到预测框之间的这些参数就是我们模型希望能够预测的值" class="headerlink" title="而anchor box到预测框之间的这些参数就是我们模型希望能够预测的值"></a>而anchor box到预测框之间的这些参数就是我们模型希望能够预测的值</h5><p>针对一个gt框，我们可以直接通过计算得到其中心坐标。通过上面的式子，我们可以得到真实中心坐标对应的$t$参数值，即成了我们训练的标签。通过巧妙的变换，我们将所有的值预测问题变成了一个预测某个0-1之间数值的问题，增强了模型的鲁棒性。</p>
<h5 id="模型功能"><a href="#模型功能" class="headerlink" title="模型功能"></a>模型功能</h5><p>样本：图像 + 每个objectness = 1的anchor box的四个参数值</p>
<p>标签：真实box计算出来的四个参数值-&gt;即表征真实box的位置</p>
<h5 id="预测标签"><a href="#预测标签" class="headerlink" title="预测标签"></a>预测标签</h5><p>抽象模型的训练集是只有objectness = 1的anchor box，数据是图像 + anchor box参数，标签是类别，训练后的目标是通过anchor box参数和图像就可以预测图像所属的标签。</p>
<h3 id="2-模型设计"><a href="#2-模型设计" class="headerlink" title="2 模型设计"></a>2 模型设计</h3><h4 id="2-1-backbone-ResNet"><a href="#2-1-backbone-ResNet" class="headerlink" title="2.1 backbone ResNet"></a>2.1 backbone ResNet</h4><h5 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h5><p>就通道数，卷积核，步长，padding，用的leakyRelu激活函数</p>
<h5 id="shortcut残差块"><a href="#shortcut残差块" class="headerlink" title="shortcut残差块"></a>shortcut残差块</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[shortcut]</span><br><span class="line"></span><br><span class="line">from=-3</span><br><span class="line"></span><br><span class="line">activation=linear</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>跳过连接与残差网络中使用的结构相似，参数 from 为-3 表示捷径层的输出会通过将之前层的和之前第三个层的输出的特征图与模块的输入相加而得出</p>
<h5 id="上采样upsample"><a href="#上采样upsample" class="headerlink" title="上采样upsample"></a>上采样upsample</h5><p>参数只有一个步幅</p>
<h5 id="路由层Route"><a href="#路由层Route" class="headerlink" title="路由层Route"></a>路由层Route</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[route]</span><br><span class="line"></span><br><span class="line">layers = -4</span><br><span class="line"></span><br><span class="line">[route]</span><br><span class="line"></span><br><span class="line">layers = -1, 61</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>路由层需要一些解释，它的参数 layers 有一个或两个值。当只有一个值时，它输出这一层通过该值索引的特征图。在我们的实验中设置为了-4，所以层级将输出路由层之前第四个层的特征图。</p>
<p>当层级有两个值时，它将返回由这两个值索引的拼接特征图。在我们的实验中为-1 和 61，因此该层级将输出从前一层级（-1）到第 61 层的特征图，并将它们按深度拼接。</p>
<p>按深度即对应位置的方块通道数增加</p>
<h5 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">x = np.random.randn(1, 3, 640, 640).astype(&#x27;float32&#x27;)</span><br><span class="line"></span><br><span class="line">x = paddle.to_tensor(x)</span><br><span class="line"></span><br><span class="line">C0, C1, C2 = backbone(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>指定输入数据的形状是(1,3,640,640)，则3个层级的输出特征图的形状分别是C0 (1, 1024, 20, 20)，C1(1,512,40,40)和C2(1,256,80,80)。</p>
<p>每个小方块区域产生K个预测框，每个预测框需要(5+C)个实数预测值，则每个像素点相对应的要有K(5+C)个实数。为了解决这一问题，对特征图进行多次卷积，并将最终的输出通道数设置为K(5+C)，即可将生成的特征图与每个预测框所需要的预测值对应起来。7</p>
<p>目的：保证为了针对输出和标签形成对应的loss。</p>
<h4 id="2-2-yolo层"><a href="#2-2-yolo层" class="headerlink" title="2.2 yolo层"></a>2.2 yolo层</h4><p>yolo层是检测层(一种head)，传入一个Backbone生成的特征图（如C0），每一个特征图表征了一种属性在指定grid cell位置的第k个预测框的第n个参数的预测值。在yolo层里输入和输出通道数不变。</p>
<p>输出的是一个route特征图和一个tip特征图，其中tip比route多经历一个卷积核为3的卷积层。然后tip特征图经过一个卷积核为1的卷积层就输出C0层的预测特征图P0.</p>
<p>x-&gt;(backbone)-&gt;C0-&gt;(yolo层)route-&gt;(conv2d)-&gt;tip-&gt;(conv2d)-&gt;P0</p>
<p>-&gt;C1</p>
<p>-&gt;C2</p>
<h3 id="正负样本标注细节：objecness标注"><a href="#正负样本标注细节：objecness标注" class="headerlink" title="正负样本标注细节：objecness标注"></a>正负样本标注细节：objecness标注</h3><p>首先基于原始图像的标注文件生成许多anchor box和相应标注。</p>
<p>Shape是[batch_size, rows, cols, anchors],就是单个图片分割成rows <em> cols个cell，每个cell生成K个anchors标注，因此单个图像一共生成rows</em>cols*K个anchor标注框。</p>
<p><strong>正样本标注</strong></p>
<p>这些标注中，每个真实框只有一个标注框与之对应，即生成的所有anchor box中与真实框IoU最大的anchor box标注为正</p>
<p><strong>负样本标注</strong></p>
<p>这些标注中，没有跟任何一个真实框匹配上的标注框都标记为负样本。</p>
<p>问题： objectness标注的是该anchor bbox包含物体的可能性，即训练时理论上正样本标注框对应的预测框应该是包含物体的，负样本标注框对应的预测框应该是不包含物体的。那么是不是<strong>所有标注为负样本的标注框对应的预测框都一定不包含物体</strong>呢？答案显然是否定的，依然有很多效果很好的预测框没有参与训练。这些框的处理规则是：</p>
<h4 id="忽略预测框中，与真实框-IoU-较大的负样本框"><a href="#忽略预测框中，与真实框-IoU-较大的负样本框" class="headerlink" title="忽略预测框中，与真实框**IoU**较大的负样本框"></a><strong>忽略预测框中，与真实框**</strong>IoU<strong>**较大的负样本框</strong></h4><p>首先选出IoU大于阈值的所有pred_box</p>
<p>选出跟真实框IoU大于指定阈值的预测，get_iou_above_thresh_inds(pred_box, gt_boxes, iou_threshold)，</p>
<p> pred_box：预测框，shape是batch_size, rows, cols, anchors，即按照特征图尺寸，每个位置都有K个预测的anchors。</p>
<p> gt_bboxes：真实框， shape相同，iou_threshold是阈值（0.7）。</p>
<p>1.取出对应位置的K个pred_box与相应位置的gt_boxes计算IoU，选出IoU&gt; iou_threshold的pred_boxes标记为1</p>
<p>2.将所有label_objectness &lt; 0.5的标注box选出来，索引为negative_indices,即没有和gt_bbox匹配上的标注框都已被标记为负样本。这些box本来是应该作为负样本参与损失函数objectness部分的构建，但是<strong>如果相应的**</strong>pred_box<strong><strong>有</strong></strong>IoU<strong>**和真实框很大的，标记为负样本显然会对模型的学习造成影响</strong>。因此这些选出来的bbox索引negative_indices 与1中得到的索引相乘，得到的就是既是没有与gt匹配上的样本，预测框又和gt有很大IoU，将这些框的索引标记为-1，不参与损失函数构建。</p>
<p>所以总结一下 yolov3在这个模块的特点是：<strong>按照经验设置**</strong>anchor <strong>box**</strong>，按照经验设置<strong>IoU</strong>阈值，按照经验忽略一些检测框。</p>
<h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><h3 id="loss-1"><a href="#loss-1" class="headerlink" title="loss"></a>loss</h3><p><img src="image-20220314163109996.png" alt="image-20220314163109996"></p>
<h3 id="3-输出变换"><a href="#3-输出变换" class="headerlink" title="3.输出变换"></a>3.输出变换</h3><h4 id="输出时3个预测图维度不同，先变换维度"><a href="#输出时3个预测图维度不同，先变换维度" class="headerlink" title="输出时3个预测图维度不同，先变换维度"></a>输出时3个预测图维度不同，先变换维度</h4><p>prediction（batch_size,第几个anchor box，bbox的几个参数）</p>
<p>从第三个维度里，原地取出x，y，confidence的预测值，取sigmoid值</p>
<p>x，y加上偏移：和grid cell的位置相关。</p>
<p>再根据w，h来调整anchor box</p>
<p>再原地将class的预测值通过sigmoid</p>
<p>最后将检测图的大小调整到和输入图像一致。</p>
<h4 id="执行nms（非极大值抑制）"><a href="#执行nms（非极大值抑制）" class="headerlink" title="执行nms（非极大值抑制）"></a>执行nms（非极大值抑制）</h4><p>取出一张图的bbox参数</p>
<p>先取出80个class的预测值，找出最大值，取出</p>
<h2 id="YOLOv4"><a href="#YOLOv4" class="headerlink" title="YOLOv4"></a>YOLOv4</h2><h3 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h3><h4 id="Cut-Mix"><a href="#Cut-Mix" class="headerlink" title="Cut Mix"></a>Cut Mix</h4><p>就是将一部分区域cut掉但不填充0像素而是随机填充训练集中的其他数据的区域像素值，分类结果按一定的比例分配，使得模型能够从一幅图像上的局部视图上识别出两个目标，提高训练的效率。</p>
<h4 id="Mosaic-Mix"><a href="#Mosaic-Mix" class="headerlink" title="Mosaic Mix"></a>Mosaic Mix</h4><p>四个图mix，可以显著减少有的模型对大batch size的需求（coco）</p>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><h4 id="backbone：CSPDarknet53"><a href="#backbone：CSPDarknet53" class="headerlink" title="backbone：CSPDarknet53"></a>backbone：CSPDarknet53</h4><p>目的：减少部分多余的梯度信息</p>
<p>作者认为现在的网络计算量大是由于在网络优化的过程中重复的梯度信息。作者将一个stage的头尾两部分的特征图集成起来，来解决这个问题，减少了至少20%的计算量，网络效果与之前相当甚至超过，而且CSP结构可以和其他网络结构结合，</p>
<p>使用了mish层作为激活函数，原因是在小于0的部分有更多梯度信息</p>
<h5 id="Weighted-Residual-Connections-WRC-："><a href="#Weighted-Residual-Connections-WRC-：" class="headerlink" title="Weighted-Residual-Connections (WRC)："></a>Weighted-Residual-Connections (WRC)：</h5><p>残差块里残差部分变成两个连续Conv-BN-ReLUs。</p>
<h5 id="Cross-mini-Batch-Normalization-CmBN"><a href="#Cross-mini-Batch-Normalization-CmBN" class="headerlink" title="Cross mini-Batch Normalization (CmBN)"></a>Cross mini-Batch Normalization (CmBN)</h5><h4 id="Neck：SPP-PAN"><a href="#Neck：SPP-PAN" class="headerlink" title="Neck：SPP + PAN"></a>Neck：SPP + PAN</h4><p>PAN其实就是在FPN自深到浅的concat之后再加上一个自浅到深的concat。</p>
<p>SPP（spatial pyramid pooling）特点：</p>
<ul>
<li><p>不管输入尺寸是怎样的，SPP可以产生固定大小的输出</p>
</li>
<li><p>使用多个窗口</p>
</li>
<li><p>SPP可以使用同一图像不同尺寸作为输入，得到同样长度的池化特征。</p>
</li>
</ul>
<p>根据输入和输入图像的尺寸来设计滑动窗口的尺寸和步长，从而保证不同的输入有相同的输出</p>
<p>保留原图片的尺寸对实验的特征提取和结果都很重要。</p>
<p>简单一点就是目标检测时把最后一个卷积特征图上ROI拿出来，经过SPP之后不同大小的ROI区域图像变成相同大小的数据输入到全连接层</p>
<p>实验证明：</p>
<p>对尺度的改变具有更强的鲁棒性，能提高准确率</p>
<p>而且是pooling层，对网络结构没有影响</p>
<h3 id="loss-2"><a href="#loss-2" class="headerlink" title="loss"></a>loss</h3><p>MSE损失函数存在的问题：将中心点坐标作为独立的变量对待</p>
<p>（1）bounding box regression损失</p>
<p>（2）置信度损失</p>
<p>（3）分类损失</p>
<h4 id="IOU-loss"><a href="#IOU-loss" class="headerlink" title="IOU loss"></a>IOU loss</h4><script type="math/tex; mode=display">L_{IOU}=1-IOU(A,B)</script><p>但这个损失函数知道bbox重叠的时候管用，不重叠的话iou=0，没有梯度</p>
<h4 id="GIOU"><a href="#GIOU" class="headerlink" title="GIOU"></a>GIOU</h4><script type="math/tex; mode=display">L_{GIOU}=1-IOU(A,B)+|C-A\bigcup B|/C</script><p>Generallized IOU loss,IOU LOSS加了一个惩罚项，惩罚项是大框框C中非AB部分面积和大框框面积的比，当然loss越小越好，具体是GIOU先增大预测框的大小来找真实框，找到后再优化，但问题在于这个找真实框的时间可能会很久，需要比较长的收敛速度</p>
<p><img src="GIOU.png" alt="image-20220305205502841"></p>
<h4 id="DIOU"><a href="#DIOU" class="headerlink" title="DIOU"></a>DIOU</h4><p>D:distance，原理在于优化anchor中心和真实框中心的距离，这样可以收敛更快</p>
<script type="math/tex; mode=display">L_{GIOU}=1-IOU(A,B)+\rho (A_{ctr},B_{ctr})/c^{2}</script><p>A : 预测框， B：真实框</p>
<p>$A_{ctr}$: 预测框中心点坐标</p>
<p>$B_{ctr}$ ：真实框中心点坐标</p>
<p>$\rho (.)$是欧式距离的计算</p>
<p>c 为 A , B 最小包围框的对角线长度</p>
<p><img src="diou.png" alt="image-20220305205556281"></p>
<p>距离越远，DIOU越接近2，距离越近，DIOU越接近0。</p>
<h4 id="CIOU"><a href="#CIOU" class="headerlink" title="CIOU"></a>CIOU</h4><p>Complete IoU Loss</p>
<p>（1）重叠面积</p>
<p>（2）中心点距离</p>
<p>（3）长宽比</p>
<script type="math/tex; mode=display">L_{GIOU}=1-IOU(A,B)+\rho (A_{ctr},B_{ctr})/c^{2}+\alpha.v</script><h2 id="YOLOv5"><a href="#YOLOv5" class="headerlink" title="YOLOv5"></a>YOLOv5</h2><h3 id="网络结构和v4基本相同"><a href="#网络结构和v4基本相同" class="headerlink" title="网络结构和v4基本相同"></a>网络结构和v4基本相同</h3><h4 id="backbone-Neck"><a href="#backbone-Neck" class="headerlink" title="backbone + Neck"></a>backbone + Neck</h4><p>和v4一样，都是CSPDarknet + PAN</p>
<h1 id="明确每个trick目的是什么"><a href="#明确每个trick目的是什么" class="headerlink" title="明确每个trick目的是什么"></a>明确每个trick目的是什么</h1><p>一般目标检测模型的pipeline显示特征提取，而后是分类器或者定位器在特征空间中寻找需要的特征，一般是用滑动窗口扫描整张图像或者扫描图相中的子集</p>
<p>模型Head主要用于最终检测部分。它在特征图上应用锚定框，并生成带有类概率、对象得分和包围框的最终输出向量。</p>
<h2 id="通用API"><a href="#通用API" class="headerlink" title="通用API"></a>通用API</h2><h3 id="绘制anchor-box"><a href="#绘制anchor-box" class="headerlink" title="绘制anchor box"></a>绘制anchor box</h3><h4 id="输入："><a href="#输入：" class="headerlink" title="输入："></a>输入：</h4><p>中心点坐标：[x, y]</p>
<p>基准长度：length</p>
<p>尺寸比例：scales</p>
<p>长宽比例：ratios</p>
<p>原图尺寸：img_height,img_width</p>
<h4 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h4><p>基准长度,尺寸,和长宽比得到anchor的长和宽</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">h = length \times scales \times \sqrt[]&#123;ratios&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w = length \times scales \div \sqrt[]&#123;ratios&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>由中心点和w，h就可以确定anchor box的坐标和大小</p>
<p>然后传入已有的坐标轴即可</p>
<h3 id="计算iou"><a href="#计算iou" class="headerlink" title="计算iou"></a>计算iou</h3><h4 id="输入：-1"><a href="#输入：-1" class="headerlink" title="输入："></a>输入：</h4><p>xywh形式或者xyxy格式的两个box，即两个长度为4的数列</p>
<h4 id="输出："><a href="#输出：" class="headerlink" title="输出："></a>输出：</h4><p>IOU数值</p>
<h4 id="功能："><a href="#功能：" class="headerlink" title="功能："></a>功能：</h4><p>思想都是先取出相交处的左上和右下点的坐标，计算出相交面积，然后就好算了。</p>
<h2 id="-1"><a href="#-1" class="headerlink" title=" "></a> </h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/01/Linux%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="篮球架上打砖块">
      <meta itemprop="description" content="Apodidae">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Upperlan">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/01/Linux%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/" class="post-title-link" itemprop="url">Linux常用操作</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-01 22:22:17" itemprop="dateCreated datePublished" datetime="2021-11-01T22:22:17+08:00">2021-11-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-03-06 13:41:53" itemprop="dateModified" datetime="2022-03-06T13:41:53+08:00">2022-03-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%B7%A5%E4%BD%9C%E5%B8%B8%E7%94%A8/" itemprop="url" rel="index"><span itemprop="name">工作常用</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="磁盘管理"><a href="#磁盘管理" class="headerlink" title="磁盘管理"></a>磁盘管理</h2><p>pwd：print work directory<br>ls : list file<br>ls -a<br>ls -ltr<br><code>mkdir &lt;directory&gt;</code>：创建目录<br><code>rmdir &lt;directory&gt;</code>:删除目录</p>
<h2 id="文件管理"><a href="#文件管理" class="headerlink" title="文件管理"></a>文件管理</h2><p>移动和剪切：<br>cp 要复制的文件名或目录 目标目录<br>mv 要移动的文件名或目录 目标目录<br>mv 要被更改的文件名 更改后的文件名<br>删除文件：<br>rm 删除的文件名<br>rm -f 要删除的文件夹名<br>打印文件：cat 要输出的文件名<br>用cat复制一个文本并重命名：cat 要被重命名的文件 &gt; 重命名后的文件<br>看一个文件的前几行： head -n num 文件名<br>对比A和B文件：diff A B</p>
<h2 id="文件压缩"><a href="#文件压缩" class="headerlink" title="文件压缩"></a>文件压缩</h2><h3 id="tar文件"><a href="#tar文件" class="headerlink" title="tar文件"></a>tar文件</h3><p>打包：tar -zcvf<br>解压: tar -zxvf</p>
<h3 id="zip文件"><a href="#zip文件" class="headerlink" title="zip文件"></a>zip文件</h3><p>打包：zip 目标压缩包的名称 被压缩的文件1 被压缩的文件2</p>
<h2 id="man命令：忘记某个命令参数的含义了"><a href="#man命令：忘记某个命令参数的含义了" class="headerlink" title="man命令：忘记某个命令参数的含义了"></a>man命令：忘记某个命令参数的含义了</h2><p>man 对应的命令<br>man 5 配置文件的详细信息了</p>
<h2 id="用户管理"><a href="#用户管理" class="headerlink" title="用户管理"></a>用户管理</h2><p>useradd 创建的用户名<br>userdel 删除的用户名<br>修改当前用户密码：passwd<br>修改其他用户密码（root）：passwd 用户名</p>
<h3 id="权限管理："><a href="#权限管理：" class="headerlink" title="权限管理："></a>权限管理：</h3><p>权限划分：-rwxrwxrwx-<br>前三位 文件归属人拥有的权限是什么<br>中间三位 归属的组，组员拥有的是什么<br>后三位 如果都不是以上人员，拥有的权限是什么<br>读权限 对应数字r=4<br>写权限 对应数字w=2<br>执行权限 对应数字x=1<br>修改权限chmod<br>chmod 750 filename<br>chmod -R 750 dirname<br>修改归属chown<br>修改归属人以及归属组：<br>chown user:group filename<br>chown -R user:group dirname</p>
<h2 id="文件修改"><a href="#文件修改" class="headerlink" title="文件修改"></a>文件修改</h2><p>sed -i ‘s/aaa/bbb/g’ filename<br>s代表替换，aaa代表替换前的原始字符，bbb代表替换后的字符，g代表的是所有文件全部修改</p>
<h3 id="删除文件内容"><a href="#删除文件内容" class="headerlink" title="删除文件内容"></a>删除文件内容</h3><p>删除前三行的内容：sed -i 1,3d filename<br>第一行到第三行全部删除，d就是删除的意思<br>删除第五行内容：sed -i 5d filename</p>
<h3 id="文本处理命令awk"><a href="#文本处理命令awk" class="headerlink" title="文本处理命令awk"></a>文本处理命令awk</h3><h4 id="按列输出"><a href="#按列输出" class="headerlink" title="按列输出"></a>按列输出</h4><p>awk ‘{print $2}’ filename //以空格为分隔符，输出第二列<br>awk ‘{print $NF}’ filename //以空格为分隔符，输出最后一列<br>awk -F ‘a’ ‘{print $2}’ filename //以a为分隔符，输出第二列<br>正则字符串匹配：<br>awk ‘$2 ~/th/ {print $2,$4}’ filename //第二列包含th字符时，输出第2，4列</p>
<h2 id="搜索查找命令"><a href="#搜索查找命令" class="headerlink" title="搜索查找命令"></a>搜索查找命令</h2><h3 id="grep"><a href="#grep" class="headerlink" title="grep"></a>grep</h3><p>grep -n ‘t[ae]st’ filename //在filename中查找所有包含tast或者test的内容<br>grep abc -rl dirname // grep查找dirname中所有包含abc内容的文件，可以搜索代码的变量</p>
<h3 id="find"><a href="#find" class="headerlink" title="find"></a>find</h3><p>find / -name filename // 从根目录下查找名为filename的文件<br>find / -name filename -type d //从根目录小查找文件名为filename的文件并且属性为目录的文件<br>find / -name filename -exec ls -l {} \; //查找文件名为filename的文件，并查看该文件的属性</p>
<h2 id="网络管理"><a href="#网络管理" class="headerlink" title="网络管理"></a>网络管理</h2><p>ping 网址 ：检测网络连通性<br>telnet  网址 端口 ：检测域名/IP和端口<br>netstat 各种网络相关信息，如网络连接，路由表，接口状态<br>ifconfig 查看网卡信息</p>
<h2 id="进程与线程"><a href="#进程与线程" class="headerlink" title="进程与线程"></a>进程与线程</h2><p>ps展示PID和cmd内容<br>ps -ef内容更丰富<br>top跳到一个新的页面，可以监控具体PID占用cpu，内存的情况<br>杀死进程<br>kill <pid><br>强制杀死<br>kill -9 <pid></p>
<h3 id="磁盘管理-1"><a href="#磁盘管理-1" class="headerlink" title="磁盘管理"></a>磁盘管理</h3><p>df命令：显示磁盘分区上可以使用的磁盘空间<br>    df -lh:以GB，MB，KB的格式显示可以使用的分区<br>du命令：显示每个文件和目录的磁盘使用空间</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/intro/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/intro/">1</a><span class="page-number current">2</span><a class="page-number" href="/intro/page/3/">3</a><a class="extend next" rel="next" href="/intro/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">篮球架上打砖块</p>
  <div class="site-description" itemprop="description">Apodidae</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">篮球架上打砖块</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 



        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '[object Object]';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
